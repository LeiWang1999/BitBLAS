[FastDlight] Apply config  {'block': [128, 64], 'warp': [64, 32], 'rstep': [64], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [128, 32], 'warp': [64, 16], 'rstep': [128], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [256, 64], 'warp': [128, 32], 'rstep': [64], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [64, 128], 'warp': [32, 64], 'rstep': [64], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [64, 64], 'warp': [32, 32], 'rstep': [64], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [128, 128], 'warp': [64, 64], 'rstep': [64], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [64, 32], 'warp': [32, 16], 'rstep': [256], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [128, 16], 'warp': [32, 16], 'rstep': [128], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [256, 32], 'warp': [128, 16], 'rstep': [128], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [256, 128], 'warp': [128, 64], 'rstep': [64], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [64, 16], 'warp': [16, 16], 'rstep': [256], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [32, 128], 'warp': [16, 64], 'rstep': [128], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [64, 256], 'warp': [32, 128], 'rstep': [64], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [32, 64], 'warp': [16, 32], 'rstep': [256], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [128, 256], 'warp': [64, 128], 'rstep': [64], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [32, 32], 'warp': [16, 16], 'rstep': [256], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [256, 16], 'warp': [64, 16], 'rstep': [128], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [16, 128], 'warp': [16, 32], 'rstep': [128], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [32, 256], 'warp': [16, 128], 'rstep': [128], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Apply config  {'block': [16, 64], 'warp': [16, 16], 'rstep': [256], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] LocalBuilder: An exception occurred  Traceback (most recent call last):
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/t-leiwang/ladder_workspace/BitBLAS/python/bitblas/base/utils.py", line 201, in _build
    rt_mod = tvm.build(mod["main"], target=arch.target)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/driver/build_module.py", line 294, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 204, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 128, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: template <typename T1, typename T2>
__device__ void decode_i2s_to_i8s(T1 *_i2s, T2 *_i8s, const int N = 16)
{
  // convert 8 int2b_t to 8 int8b_t -> 2 int32
  uint *i8s = reinterpret_cast<uint *>(_i8s);

  // i2s = {e7,e6,e5,e4,e3,e2,e1,e0}
  // also require interleave {e7,e3,e6,e2,e5,e1,e4,e0}
  uint const i2s = *_i2s;

  // First, we extract the i4s and construct an intermediate fp16 number.
  static constexpr uint immLut = (0xf0 & 0xcc) | 0xaa;     // 0b11101010
  static constexpr uint BOTTOM_MASK = 0x03030303;          // 0xf -> 0b11 select 0,3
  static constexpr uint I4s_TO_I8s_MAGIC_NUM = 0x00000000; // 1024

#pragma unroll
  for (int i = 0; i < (N / 2); i++)
  {
    asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                 : "=r"(i8s[i])
                 : "r"(i2s >> (2 * i)), "n"(BOTTOM_MASK), "n"(I4s_TO_I8s_MAGIC_NUM), "n"(immLut));
  }
}
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 610)
#include <sm_61_intrinsics.h>


#if defined(__CUDACC_RTC__)
#define __SM_61_INTRINSICS_DECL__ __device__
#else /* !__CUDACC_RTC__ */
#define __SM_61_INTRINSICS_DECL__ static __device__ __inline__
#endif /* __CUDACC_RTC__ */

#ifndef __CUDA_ARCH__
#define __DEF_IF_HOST { }
#else  /* !__CUDA_ARCH__ */
#define __DEF_IF_HOST ;
#endif /* __CUDA_ARCH__ */

__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) __DEF_IF_HOST
__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) __DEF_IF_HOST

#undef __DEF_IF_HOST

#if !defined(__CUDACC_RTC__) && defined(__CUDA_ARCH__)
__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) {
    int ret;
    asm volatile ("dp4a.u32.s32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}

__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) {
    int ret;
    asm volatile ("dp4a.s32.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}
#endif /* !__CUDACC_RTC__ && defined(__CUDA_ARCH__) */

#undef __SM_61_INTRINSICS_DECL__

#endif
__forceinline__ __device__ unsigned int
cast_smem_ptr_to_int(const void* const smem_ptr)
{
  unsigned int smem_int;
  asm volatile ("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; cvt.u32.u64 %0, smem_int; }"
    : "=r"(smem_int) : "l"(smem_ptr));
  return smem_int;
}

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif

#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ == 800) 
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 1
#else
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 0
#endif
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C);
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C) {

        const int MAX_BLOCK_N = 10;
        const auto baseBlockIdx = blockIdx.x + gridDim.x *blockIdx.y;
        const auto totalPanel = (gridDim.x * gridDim.y +MAX_BLOCK_N * gridDim.x - 1) / (MAX_BLOCK_N * gridDim.x);
        const auto totalBlock = gridDim.x * gridDim.y;
        const auto panelIdx = baseBlockIdx / (MAX_BLOCK_N *gridDim.x);
        const auto strideLd = panelIdx + 1 < totalPanel ?MAX_BLOCK_N : (totalBlock - panelIdx * (MAX_BLOCK_N *gridDim.x)) / gridDim.x;
        const auto bx = (panelIdx & 1) ? gridDim.x -(baseBlockIdx - panelIdx * MAX_BLOCK_N * gridDim.x) /strideLd - 1 : (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) / strideLd;
        const auto by = (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) % strideLd + panelIdx * MAX_BLOCK_N;
        const auto bz = blockIdx.z;
        const dim3 blockIdx(bx, by, bz);
      __shared__ uchar buf_shmem[75776];
  int C_reindex_shared_warp[128];
  int B_local[1];
  int4 B_reindex_reindex_local[1];
  signed char A_reindex_shared_warp[128];
  signed char B_reindex_reindex_shared_warp[32];
  int B_local_1[1];
  int4 B_reindex_reindex_local_1[1];
  signed char A_reindex_shared_warp_1[128];
  signed char B_reindex_reindex_shared_warp_1[32];
  for (int var = 0; var < 1; ++var) {
    for (int ax1_0_3_init = 0; ax1_0_3_init < 8; ++ax1_0_3_init) {
      for (int ax2_0_3_init = 0; ax2_0_3_init < 2; ++ax2_0_3_init) {
        for (int i = 0; i < 8; ++i) {
C_reindex_shared_warp[((ax1_0_3_init * 16) + (ax2_0_3_init * 8)) + i] = 0.0;}
;
      }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_fused_2 = 0; ax0_ax1_ax2_fused_2 < 8; ++ax0_ax1_ax2_fused_2) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 6144))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 6144)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((int)blockIdx.y) * 4194304) + (((int)threadIdx.y) * 2097152)) + (((int)threadIdx.z) * 1048576)) + (ax0_ax1_ax2_fused_2 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + ((((int)threadIdx.x) & 3) * 16)))), "n"(16)
    );
  }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_fused_0 = 0; ax0_ax1_ax2_ax3_fused_0 < 1; ++ax0_ax1_ax2_ax3_fused_0) {
      if (((int)threadIdx.z) < 1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((int)threadIdx.y) * 512) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((int)threadIdx.y) * 512) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((int)blockIdx.z) * 2097152) + (((int)blockIdx.x) * 262144)) + (((int)threadIdx.z) * 262144)) + (((int)threadIdx.y) * 131072)) + ((((int)threadIdx.x) >> 4) * 65536)) + ((((int)threadIdx.x) & 15) * 16)))), "n"(16)
    );
  }
      }
    }
__asm__ __volatile__("cp.async.commit_group;");

    for (int ax3_0_0 = 0; ax3_0_0 < 255; ++ax3_0_0) {
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_fused_2_1 = 0; ax0_ax1_ax2_fused_2_1 < 8; ++ax0_ax1_ax2_fused_2_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2_1 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 6144))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2_1 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 6144)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int)blockIdx.y) * 4194304) + (((int)threadIdx.y) * 2097152)) + (((int)threadIdx.z) * 1048576)) + (ax0_ax1_ax2_fused_2_1 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (ax3_0_0 * 64)) + ((((int)threadIdx.x) & 3) * 16)) + 64))), "n"(16)
    );
  }
      }
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_fused_0_1 = 0; ax0_ax1_ax2_ax3_fused_0_1 < 1; ++ax0_ax1_ax2_ax3_fused_0_1) {
        if (((int)threadIdx.z) < 1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((ax3_0_0 + 1) & 1) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((ax3_0_0 + 1) & 1) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int)blockIdx.z) * 2097152) + (((int)blockIdx.x) * 262144)) + (((int)threadIdx.z) * 262144)) + (((int)threadIdx.y) * 131072)) + ((((int)threadIdx.x) >> 4) * 65536)) + (ax3_0_0 * 256)) + ((((int)threadIdx.x) & 15) * 16)) + 256))), "n"(16)
    );
  }
        }
      }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 1;");

      __syncthreads();
      for (int ax1_ax2_ax3_ax4_0_fused_0 = 0; ax1_ax2_ax3_ax4_0_fused_0 < 2; ++ax1_ax2_ax3_ax4_0_fused_0) {
        B_local[0] = *(int*)(((signed char*)buf_shmem) + ((((((ax3_0_0 & 1) * 1024) + (ax1_ax2_ax3_ax4_0_fused_0 * 512)) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)));
        decode_i2s_to_i8s(B_local, B_reindex_reindex_local, 16);
        *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 2048)) = B_reindex_reindex_local[0];
      }
      __syncthreads();
      for (int ax3_0_1 = 0; ax3_0_1 < 2; ++ax3_0_1) {
        for (int ax1_0 = 0; ax1_0 < 8; ++ax1_0) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (ax1_0 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 6144)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (ax1_0 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 6144)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax1 = 0; ax1 < 2; ++ax1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 2048) + (ax1 * 1024)) + (ax3_0_1 * 512)) + 2048)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 2048) + (ax1 * 1024)) + (ax3_0_1 * 512)) + 2048)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax1_0_3 = 0; ax1_0_3 < 8; ++ax1_0_3) {
          for (int ax2_0_3 = 0; ax2_0_3 < 2; ++ax2_0_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 16) + (ax2_0_3 * 8))))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 16) + (ax2_0_3 * 8))))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 16) + (ax2_0_3 * 8))))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 16) + (ax2_0_3 * 8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 16) + (ax2_0_3 * 8))))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 16) + (ax2_0_3 * 8))))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 16) + (ax2_0_3 * 8))))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 16) + (ax2_0_3 * 8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 16) + (ax2_0_3 * 8)) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 16) + (ax2_0_3 * 8)) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 16) + (ax2_0_3 * 8)) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 16) + (ax2_0_3 * 8)) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 16) + (ax2_0_3 * 8)) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 16) + (ax2_0_3 * 8)) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 16) + (ax2_0_3 * 8)) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 16) + (ax2_0_3 * 8)) + 4)))[3]));
  }
          }
        }
      }
    }
__asm__ __volatile__("cp.async.wait_group 0;");

    __syncthreads();
    for (int ax1_ax2_ax3_ax4_0_fused_0_1 = 0; ax1_ax2_ax3_ax4_0_fused_0_1 < 2; ++ax1_ax2_ax3_ax4_0_fused_0_1) {
      B_local_1[0] = *(int*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 512) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)) + 1024));
      decode_i2s_to_i8s(B_local_1, B_reindex_reindex_local_1, 16);
      *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 2048)) = B_reindex_reindex_local_1[0];
    }
    __syncthreads();
    for (int ax3_0_1_1 = 0; ax3_0_1_1 < 2; ++ax3_0_1_1) {
      for (int ax1_0_1 = 0; ax1_0_1 < 8; ++ax1_0_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 8192) + (ax1_0_1 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 22528)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 8192) + (ax1_0_1 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 22528)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax1_1 = 0; ax1_1 < 2; ++ax1_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 2048) + (ax1_1 * 1024)) + (ax3_0_1_1 * 512)) + 2048)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 2048) + (ax1_1 * 1024)) + (ax3_0_1_1 * 512)) + 2048)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax1_0_3_1 = 0; ax1_0_3_1 < 8; ++ax1_0_3_1) {
        for (int ax2_0_3_1 = 0; ax2_0_3_1 < 2; ++ax2_0_3_1) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8))))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8))))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8))))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8))))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8))))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8))))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8)) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8)) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8)) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8)) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8)) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8)) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8)) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 16) + (ax2_0_3_1 * 8)) + 4)))[3]));
  }
        }
      }
    }
    for (int ax0 = 0; ax0 < 8; ++ax0) {
      __syncthreads();
      for (int ax1_2 = 0; ax1_2 < 2; ++ax1_2) {
        for (int local_id = 0; local_id < 8; ++local_id) {
(&(((int*)buf_shmem)[((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 512)) + (ax1_2 * 256)) + 9728)]))[((((((local_id % 4) / 2) * 8) + (threadIdx.x / 4)) * 16) + ((((local_id / 4) * 8) + ((threadIdx.x % 4) * 2)) + (local_id % 2)))] = C_reindex_shared_warp[((ax0 * 16) + (ax1_2 * 8)) + local_id];
}
;
      }
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_ax4_fused_0 = 0; ax0_ax1_ax2_ax3_ax4_fused_0 < 4; ++ax0_ax1_ax2_ax3_ax4_fused_0) {
        *(int4*)(C + ((((((((((((int)blockIdx.y) * 4194304) + (((int)threadIdx.y) * 2097152)) + (ax0 * 262144)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 & 1) * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (((int)blockIdx.z) * 512)) + (((int)blockIdx.x) * 64)) + (((int)threadIdx.z) * 32)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4))) = *(int4*)(((int*)buf_shmem) + (((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 512)) + (ax0_ax1_ax2_ax3_ax4_fused_0 * 128)) + (((int)threadIdx.x) * 4)) + 9728));
      }
    }
  }
}


Compilation error:
/tmp/tmph1ixa0cq/tvm_kernels.cu(52): warning #177-D: function "__dp4a(int, unsigned int, int)" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/tmp/tmph1ixa0cq/tvm_kernels.cu(46): warning #177-D: function "__dp4a(unsigned int, int, int)" was declared but never referenced

ptxas error   : Entry function 'default_function_kernel' uses too much shared data (0x12800 bytes, 0xc000 max)


[FastDlight] LocalBuilder: An exception occurred  Traceback (most recent call last):
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/t-leiwang/ladder_workspace/BitBLAS/python/bitblas/base/utils.py", line 201, in _build
    rt_mod = tvm.build(mod["main"], target=arch.target)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/driver/build_module.py", line 294, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 204, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 128, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: template <typename T1, typename T2>
__device__ void decode_i2s_to_i8s(T1 *_i2s, T2 *_i8s, const int N = 16)
{
  // convert 8 int2b_t to 8 int8b_t -> 2 int32
  uint *i8s = reinterpret_cast<uint *>(_i8s);

  // i2s = {e7,e6,e5,e4,e3,e2,e1,e0}
  // also require interleave {e7,e3,e6,e2,e5,e1,e4,e0}
  uint const i2s = *_i2s;

  // First, we extract the i4s and construct an intermediate fp16 number.
  static constexpr uint immLut = (0xf0 & 0xcc) | 0xaa;     // 0b11101010
  static constexpr uint BOTTOM_MASK = 0x03030303;          // 0xf -> 0b11 select 0,3
  static constexpr uint I4s_TO_I8s_MAGIC_NUM = 0x00000000; // 1024

#pragma unroll
  for (int i = 0; i < (N / 2); i++)
  {
    asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                 : "=r"(i8s[i])
                 : "r"(i2s >> (2 * i)), "n"(BOTTOM_MASK), "n"(I4s_TO_I8s_MAGIC_NUM), "n"(immLut));
  }
}
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 610)
#include <sm_61_intrinsics.h>


#if defined(__CUDACC_RTC__)
#define __SM_61_INTRINSICS_DECL__ __device__
#else /* !__CUDACC_RTC__ */
#define __SM_61_INTRINSICS_DECL__ static __device__ __inline__
#endif /* __CUDACC_RTC__ */

#ifndef __CUDA_ARCH__
#define __DEF_IF_HOST { }
#else  /* !__CUDA_ARCH__ */
#define __DEF_IF_HOST ;
#endif /* __CUDA_ARCH__ */

__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) __DEF_IF_HOST
__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) __DEF_IF_HOST

#undef __DEF_IF_HOST

#if !defined(__CUDACC_RTC__) && defined(__CUDA_ARCH__)
__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) {
    int ret;
    asm volatile ("dp4a.u32.s32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}

__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) {
    int ret;
    asm volatile ("dp4a.s32.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}
#endif /* !__CUDACC_RTC__ && defined(__CUDA_ARCH__) */

#undef __SM_61_INTRINSICS_DECL__

#endif
__forceinline__ __device__ unsigned int
cast_smem_ptr_to_int(const void* const smem_ptr)
{
  unsigned int smem_int;
  asm volatile ("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; cvt.u32.u64 %0, smem_int; }"
    : "=r"(smem_int) : "l"(smem_ptr));
  return smem_int;
}

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif

#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ == 800) 
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 1
#else
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 0
#endif
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C);
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C) {

        const int MAX_BLOCK_N = 10;
        const auto baseBlockIdx = blockIdx.x + gridDim.x *blockIdx.y;
        const auto totalPanel = (gridDim.x * gridDim.y +MAX_BLOCK_N * gridDim.x - 1) / (MAX_BLOCK_N * gridDim.x);
        const auto totalBlock = gridDim.x * gridDim.y;
        const auto panelIdx = baseBlockIdx / (MAX_BLOCK_N *gridDim.x);
        const auto strideLd = panelIdx + 1 < totalPanel ?MAX_BLOCK_N : (totalBlock - panelIdx * (MAX_BLOCK_N *gridDim.x)) / gridDim.x;
        const auto bx = (panelIdx & 1) ? gridDim.x -(baseBlockIdx - panelIdx * MAX_BLOCK_N * gridDim.x) /strideLd - 1 : (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) / strideLd;
        const auto by = (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) % strideLd + panelIdx * MAX_BLOCK_N;
        const auto bz = blockIdx.z;
        const dim3 blockIdx(bx, by, bz);
      __shared__ uchar buf_shmem[55296];
  int C_reindex_shared_warp[32];
  int B_local[1];
  int4 B_reindex_reindex_local[1];
  signed char A_reindex_shared_warp[64];
  signed char B_reindex_reindex_shared_warp[16];
  int B_local_1[1];
  int4 B_reindex_reindex_local_1[1];
  signed char A_reindex_shared_warp_1[64];
  signed char B_reindex_reindex_shared_warp_1[16];
  for (int var = 0; var < 1; ++var) {
    for (int ax1_0_3_init = 0; ax1_0_3_init < 4; ++ax1_0_3_init) {
      for (int i = 0; i < 8; ++i) {
C_reindex_shared_warp[(ax1_0_3_init * 8) + i] = 0.0;}
;
    }
    #pragma unroll
    for (int ax0_ax1_ax2_fused_2 = 0; ax0_ax1_ax2_fused_2 < 8; ++ax0_ax1_ax2_fused_2) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((ax0_ax1_ax2_fused_2 & 3) * 4) + (((int)threadIdx.x) >> 3))) * 16)) + 22528))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((ax0_ax1_ax2_fused_2 & 3) * 4) + (((int)threadIdx.x) >> 3))) * 16)) + 22528)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((int)blockIdx.y) * 2097152) + (((int)threadIdx.y) * 1048576)) + (((int)threadIdx.z) * 524288)) + (ax0_ax1_ax2_fused_2 * 65536)) + ((((int)threadIdx.x) >> 3) * 16384)) + ((((int)threadIdx.x) & 7) * 16)))), "n"(16)
    );
  }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_fused_0 = 0; ax0_ax1_ax2_ax3_fused_0 < 1; ++ax0_ax1_ax2_ax3_fused_0) {
      if (((int)threadIdx.z) < 1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((int)threadIdx.y) * 512) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((int)threadIdx.y) * 512) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + (((((((int)blockIdx.z) * 1048576) + (((int)blockIdx.x) * 131072)) + (((int)threadIdx.z) * 131072)) + (((int)threadIdx.y) * 65536)) + (((int)threadIdx.x) * 16)))), "n"(16)
    );
  }
      }
    }
__asm__ __volatile__("cp.async.commit_group;");

    for (int ax3_0_0 = 0; ax3_0_0 < 127; ++ax3_0_0) {
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_fused_2_1 = 0; ax0_ax1_ax2_fused_2_1 < 8; ++ax0_ax1_ax2_fused_2_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((ax0_ax1_ax2_fused_2_1 & 3) * 4) + (((int)threadIdx.x) >> 3))) * 16)) + 22528))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((ax0_ax1_ax2_fused_2_1 & 3) * 4) + (((int)threadIdx.x) >> 3))) * 16)) + 22528)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int)blockIdx.y) * 2097152) + (((int)threadIdx.y) * 1048576)) + (((int)threadIdx.z) * 524288)) + (ax0_ax1_ax2_fused_2_1 * 65536)) + ((((int)threadIdx.x) >> 3) * 16384)) + (ax3_0_0 * 128)) + ((((int)threadIdx.x) & 7) * 16)) + 128))), "n"(16)
    );
  }
      }
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_fused_0_1 = 0; ax0_ax1_ax2_ax3_fused_0_1 < 1; ++ax0_ax1_ax2_ax3_fused_0_1) {
        if (((int)threadIdx.z) < 1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((ax3_0_0 + 1) & 1) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((ax3_0_0 + 1) & 1) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + (((((((((int)blockIdx.z) * 1048576) + (((int)blockIdx.x) * 131072)) + (((int)threadIdx.z) * 131072)) + (((int)threadIdx.y) * 65536)) + (ax3_0_0 * 512)) + (((int)threadIdx.x) * 16)) + 512))), "n"(16)
    );
  }
        }
      }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 1;");

      __syncthreads();
      for (int ax1_ax2_ax3_ax4_0_fused_0 = 0; ax1_ax2_ax3_ax4_0_fused_0 < 2; ++ax1_ax2_ax3_ax4_0_fused_0) {
        B_local[0] = *(int*)(((signed char*)buf_shmem) + ((((((ax3_0_0 & 1) * 1024) + (ax1_ax2_ax3_ax4_0_fused_0 * 512)) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)));
        decode_i2s_to_i8s(B_local, B_reindex_reindex_local, 16);
        *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 2048)) = B_reindex_reindex_local[0];
      }
      __syncthreads();
      for (int ax3_0_1 = 0; ax3_0_1 < 4; ++ax3_0_1) {
        for (int ax1_0 = 0; ax1_0 < 4; ++ax1_0) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (ax1_0 * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 22528)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (ax1_0 * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 22528)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[3])
      : "r"(addr)
    );
  }
        }

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((int)threadIdx.z) * 2048) + (ax3_0_1 * 512)) + 2048)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((int)threadIdx.z) * 2048) + (ax3_0_1 * 512)) + 2048)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[3])
      : "r"(addr)
    );
  }
        for (int ax1_0_3 = 0; ax1_0_3 < 4; ++ax1_0_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[0]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[1]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[2]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[0]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[2]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 8))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 8))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[3]));
  }
        }
      }
    }
__asm__ __volatile__("cp.async.wait_group 0;");

    __syncthreads();
    for (int ax1_ax2_ax3_ax4_0_fused_0_1 = 0; ax1_ax2_ax3_ax4_0_fused_0_1 < 2; ++ax1_ax2_ax3_ax4_0_fused_0_1) {
      B_local_1[0] = *(int*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 512) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)) + 1024));
      decode_i2s_to_i8s(B_local_1, B_reindex_reindex_local_1, 16);
      *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 2048)) = B_reindex_reindex_local_1[0];
    }
    __syncthreads();
    for (int ax3_0_1_1 = 0; ax3_0_1_1 < 4; ++ax3_0_1_1) {
      for (int ax1_0_1 = 0; ax1_0_1 < 4; ++ax1_0_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 8192) + (ax1_0_1 * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 38912)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 8192) + (ax1_0_1 * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 38912)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((int)threadIdx.z) * 2048) + (ax3_0_1_1 * 512)) + 2048)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((int)threadIdx.z) * 2048) + (ax3_0_1_1 * 512)) + 2048)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[3])
      : "r"(addr)
    );
  }
      for (int ax1_0_3_1 = 0; ax1_0_3_1 < 4; ++ax1_0_3_1) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[0]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[1]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[2]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[0]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[2]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 8))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 8))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[3]));
  }
      }
    }
    for (int ax0 = 0; ax0 < 4; ++ax0) {
      for (int local_id = 0; local_id < 8; ++local_id) {
(&(((int*)buf_shmem)[((((((int)threadIdx.y) * 2048) + (ax0 * 512)) + (((int)threadIdx.z) * 256)) + 1536)]))[((((((local_id % 4) / 2) * 8) + (threadIdx.x / 4)) * 16) + ((((local_id / 4) * 8) + ((threadIdx.x % 4) * 2)) + (local_id % 2)))] = C_reindex_shared_warp[(ax0 * 8) + local_id];
}
;
    }
    __syncthreads();
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_ax4_fused_0 = 0; ax0_ax1_ax2_ax3_ax4_fused_0 < 8; ++ax0_ax1_ax2_ax3_ax4_fused_0) {
      *(int4*)(C + ((((((((((int)blockIdx.y) * 2097152) + (((int)threadIdx.y) * 1048576)) + (ax0_ax1_ax2_ax3_ax4_fused_0 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (((int)blockIdx.z) * 256)) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.z) * 16)) + ((((int)threadIdx.x) & 3) * 4))) = *(int4*)(((int*)buf_shmem) + ((((((((int)threadIdx.y) * 2048) + ((ax0_ax1_ax2_ax3_ax4_fused_0 >> 1) * 512)) + (((int)threadIdx.z) * 256)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 & 1) * 128)) + (((int)threadIdx.x) * 4)) + 1536));
    }
  }
}


Compilation error:
/tmp/tmpx92caiz8/tvm_kernels.cu(52): warning #177-D: function "__dp4a(int, unsigned int, int)" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/tmp/tmpx92caiz8/tvm_kernels.cu(46): warning #177-D: function "__dp4a(unsigned int, int, int)" was declared but never referenced

ptxas error   : Entry function 'default_function_kernel' uses too much shared data (0xd800 bytes, 0xc000 max)


[FastDlight] LocalBuilder: An exception occurred  Traceback (most recent call last):
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/t-leiwang/ladder_workspace/BitBLAS/python/bitblas/base/utils.py", line 201, in _build
    rt_mod = tvm.build(mod["main"], target=arch.target)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/driver/build_module.py", line 294, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 204, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 128, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: template <typename T1, typename T2>
__device__ void decode_i2s_to_i8s(T1 *_i2s, T2 *_i8s, const int N = 16)
{
  // convert 8 int2b_t to 8 int8b_t -> 2 int32
  uint *i8s = reinterpret_cast<uint *>(_i8s);

  // i2s = {e7,e6,e5,e4,e3,e2,e1,e0}
  // also require interleave {e7,e3,e6,e2,e5,e1,e4,e0}
  uint const i2s = *_i2s;

  // First, we extract the i4s and construct an intermediate fp16 number.
  static constexpr uint immLut = (0xf0 & 0xcc) | 0xaa;     // 0b11101010
  static constexpr uint BOTTOM_MASK = 0x03030303;          // 0xf -> 0b11 select 0,3
  static constexpr uint I4s_TO_I8s_MAGIC_NUM = 0x00000000; // 1024

#pragma unroll
  for (int i = 0; i < (N / 2); i++)
  {
    asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                 : "=r"(i8s[i])
                 : "r"(i2s >> (2 * i)), "n"(BOTTOM_MASK), "n"(I4s_TO_I8s_MAGIC_NUM), "n"(immLut));
  }
}
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 610)
#include <sm_61_intrinsics.h>


#if defined(__CUDACC_RTC__)
#define __SM_61_INTRINSICS_DECL__ __device__
#else /* !__CUDACC_RTC__ */
#define __SM_61_INTRINSICS_DECL__ static __device__ __inline__
#endif /* __CUDACC_RTC__ */

#ifndef __CUDA_ARCH__
#define __DEF_IF_HOST { }
#else  /* !__CUDA_ARCH__ */
#define __DEF_IF_HOST ;
#endif /* __CUDA_ARCH__ */

__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) __DEF_IF_HOST
__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) __DEF_IF_HOST

#undef __DEF_IF_HOST

#if !defined(__CUDACC_RTC__) && defined(__CUDA_ARCH__)
__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) {
    int ret;
    asm volatile ("dp4a.u32.s32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}

__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) {
    int ret;
    asm volatile ("dp4a.s32.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}
#endif /* !__CUDACC_RTC__ && defined(__CUDA_ARCH__) */

#undef __SM_61_INTRINSICS_DECL__

#endif
__forceinline__ __device__ unsigned int
cast_smem_ptr_to_int(const void* const smem_ptr)
{
  unsigned int smem_int;
  asm volatile ("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; cvt.u32.u64 %0, smem_int; }"
    : "=r"(smem_int) : "l"(smem_ptr));
  return smem_int;
}

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif

#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ == 800) 
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 1
#else
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 0
#endif
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C);
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C) {

        const int MAX_BLOCK_N = 10;
        const auto baseBlockIdx = blockIdx.x + gridDim.x *blockIdx.y;
        const auto totalPanel = (gridDim.x * gridDim.y +MAX_BLOCK_N * gridDim.x - 1) / (MAX_BLOCK_N * gridDim.x);
        const auto totalBlock = gridDim.x * gridDim.y;
        const auto panelIdx = baseBlockIdx / (MAX_BLOCK_N *gridDim.x);
        const auto strideLd = panelIdx + 1 < totalPanel ?MAX_BLOCK_N : (totalBlock - panelIdx * (MAX_BLOCK_N *gridDim.x)) / gridDim.x;
        const auto bx = (panelIdx & 1) ? gridDim.x -(baseBlockIdx - panelIdx * MAX_BLOCK_N * gridDim.x) /strideLd - 1 : (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) / strideLd;
        const auto by = (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) % strideLd + panelIdx * MAX_BLOCK_N;
        const auto bz = blockIdx.z;
        const dim3 blockIdx(bx, by, bz);
      __shared__ uchar buf_shmem[69632];
  int C_reindex_shared_warp[128];
  int B_local[1];
  int4 B_reindex_reindex_local[1];
  signed char A_reindex_shared_warp[64];
  signed char B_reindex_reindex_shared_warp[64];
  int B_local_1[1];
  int4 B_reindex_reindex_local_1[1];
  signed char A_reindex_shared_warp_1[64];
  signed char B_reindex_reindex_shared_warp_1[64];
  for (int var = 0; var < 1; ++var) {
    for (int ax1_0_3_init = 0; ax1_0_3_init < 4; ++ax1_0_3_init) {
      for (int ax2_0_3_init = 0; ax2_0_3_init < 4; ++ax2_0_3_init) {
        for (int i = 0; i < 8; ++i) {
C_reindex_shared_warp[((ax1_0_3_init * 32) + (ax2_0_3_init * 8)) + i] = 0.0;}
;
      }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_fused_2 = 0; ax0_ax1_ax2_fused_2 < 4; ++ax0_ax1_ax2_fused_2) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((int)threadIdx.y) * 4096) + (((int)threadIdx.z) * 2048)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 12288))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((int)threadIdx.y) * 4096) + (((int)threadIdx.z) * 2048)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 12288)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((int)blockIdx.y) * 2097152) + (((int)threadIdx.y) * 1048576)) + (((int)threadIdx.z) * 524288)) + (ax0_ax1_ax2_fused_2 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + ((((int)threadIdx.x) & 3) * 16)))), "n"(16)
    );
  }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_fused_0 = 0; ax0_ax1_ax2_ax3_fused_0 < 1; ++ax0_ax1_ax2_ax3_fused_0) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((int)threadIdx.z) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((int)threadIdx.z) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((int)blockIdx.z) * 4194304) + (((int)blockIdx.x) * 524288)) + (((int)threadIdx.z) * 262144)) + (((int)threadIdx.y) * 131072)) + ((((int)threadIdx.x) >> 4) * 65536)) + ((((int)threadIdx.x) & 15) * 16)))), "n"(16)
    );
  }
    }
__asm__ __volatile__("cp.async.commit_group;");

    for (int ax3_0_0 = 0; ax3_0_0 < 255; ++ax3_0_0) {
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_fused_2_1 = 0; ax0_ax1_ax2_fused_2_1 < 4; ++ax0_ax1_ax2_fused_2_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 8192) + (((int)threadIdx.y) * 4096)) + (((int)threadIdx.z) * 2048)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2_1 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 12288))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 8192) + (((int)threadIdx.y) * 4096)) + (((int)threadIdx.z) * 2048)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2_1 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 12288)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int)blockIdx.y) * 2097152) + (((int)threadIdx.y) * 1048576)) + (((int)threadIdx.z) * 524288)) + (ax0_ax1_ax2_fused_2_1 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (ax3_0_0 * 64)) + ((((int)threadIdx.x) & 3) * 16)) + 64))), "n"(16)
    );
  }
      }
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_fused_0_1 = 0; ax0_ax1_ax2_ax3_fused_0_1 < 1; ++ax0_ax1_ax2_ax3_fused_0_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((ax3_0_0 + 1) & 1) * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((ax3_0_0 + 1) & 1) * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int)blockIdx.z) * 4194304) + (((int)blockIdx.x) * 524288)) + (((int)threadIdx.z) * 262144)) + (((int)threadIdx.y) * 131072)) + ((((int)threadIdx.x) >> 4) * 65536)) + (ax3_0_0 * 256)) + ((((int)threadIdx.x) & 15) * 16)) + 256))), "n"(16)
    );
  }
      }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 1;");

      __syncthreads();
      for (int ax1_ax2_ax3_ax4_0_fused_0 = 0; ax1_ax2_ax3_ax4_0_fused_0 < 4; ++ax1_ax2_ax3_ax4_0_fused_0) {
        B_local[0] = *(int*)(((signed char*)buf_shmem) + ((((((ax3_0_0 & 1) * 2048) + (ax1_ax2_ax3_ax4_0_fused_0 * 512)) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)));
        decode_i2s_to_i8s(B_local, B_reindex_reindex_local, 16);
        *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 4096)) = B_reindex_reindex_local[0];
      }
      __syncthreads();
      for (int ax3_0_1 = 0; ax3_0_1 < 2; ++ax3_0_1) {
        for (int ax1_0 = 0; ax1_0 < 4; ++ax1_0) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 8192) + (((int)threadIdx.y) * 4096)) + (ax1_0 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 12288)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 8192) + (((int)threadIdx.y) * 4096)) + (ax1_0 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 12288)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax1 = 0; ax1 < 4; ++ax1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 4096) + (ax1 * 1024)) + (ax3_0_1 * 512)) + 4096)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 4096) + (ax1 * 1024)) + (ax3_0_1 * 512)) + 4096)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax1_0_3 = 0; ax1_0_3 < 4; ++ax1_0_3) {
          for (int ax2_0_3 = 0; ax2_0_3 < 4; ++ax2_0_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[3]));
  }
          }
        }
      }
    }
__asm__ __volatile__("cp.async.wait_group 0;");

    __syncthreads();
    for (int ax1_ax2_ax3_ax4_0_fused_0_1 = 0; ax1_ax2_ax3_ax4_0_fused_0_1 < 4; ++ax1_ax2_ax3_ax4_0_fused_0_1) {
      B_local_1[0] = *(int*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 512) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)) + 2048));
      decode_i2s_to_i8s(B_local_1, B_reindex_reindex_local_1, 16);
      *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 4096)) = B_reindex_reindex_local_1[0];
    }
    __syncthreads();
    for (int ax3_0_1_1 = 0; ax3_0_1_1 < 2; ++ax3_0_1_1) {
      for (int ax1_0_1 = 0; ax1_0_1 < 4; ++ax1_0_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 4096) + (ax1_0_1 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 20480)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 4096) + (ax1_0_1 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 20480)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax1_1 = 0; ax1_1 < 4; ++ax1_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 4096) + (ax1_1 * 1024)) + (ax3_0_1_1 * 512)) + 4096)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 4096) + (ax1_1 * 1024)) + (ax3_0_1_1 * 512)) + 4096)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax1_0_3_1 = 0; ax1_0_3_1 < 4; ++ax1_0_3_1) {
        for (int ax2_0_3_1 = 0; ax2_0_3_1 < 4; ++ax2_0_3_1) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[3]));
  }
        }
      }
    }
    for (int ax0 = 0; ax0 < 4; ++ax0) {
      __syncthreads();
      for (int ax1_2 = 0; ax1_2 < 4; ++ax1_2) {
        for (int local_id = 0; local_id < 8; ++local_id) {
(&(((int*)buf_shmem)[((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 1024)) + (ax1_2 * 256)) + 7168)]))[((((((local_id % 4) / 2) * 8) + (threadIdx.x / 4)) * 16) + ((((local_id / 4) * 8) + ((threadIdx.x % 4) * 2)) + (local_id % 2)))] = C_reindex_shared_warp[((ax0 * 32) + (ax1_2 * 8)) + local_id];
}
;
      }
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_ax4_fused_0 = 0; ax0_ax1_ax2_ax3_ax4_fused_0 < 8; ++ax0_ax1_ax2_ax3_ax4_fused_0) {
        *(int4*)(C + ((((((((((((int)blockIdx.y) * 2097152) + (((int)threadIdx.y) * 1048576)) + (ax0 * 262144)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 & 1) * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (((int)blockIdx.z) * 1024)) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.z) * 64)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4))) = *(int4*)(((int*)buf_shmem) + (((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_ax3_ax4_fused_0 * 128)) + (((int)threadIdx.x) * 4)) + 7168));
      }
    }
  }
}


Compilation error:
/tmp/tmp6v_wuey1/tvm_kernels.cu(52): warning #177-D: function "__dp4a(int, unsigned int, int)" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/tmp/tmp6v_wuey1/tvm_kernels.cu(46): warning #177-D: function "__dp4a(unsigned int, int, int)" was declared but never referenced

ptxas error   : Entry function 'default_function_kernel' uses too much shared data (0x11000 bytes, 0xc000 max)


[FastDlight] LocalBuilder: An exception occurred  Traceback (most recent call last):
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/t-leiwang/ladder_workspace/BitBLAS/python/bitblas/base/utils.py", line 201, in _build
    rt_mod = tvm.build(mod["main"], target=arch.target)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/driver/build_module.py", line 294, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 204, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 128, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: template <typename T1, typename T2>
__device__ void decode_i2s_to_i8s(T1 *_i2s, T2 *_i8s, const int N = 16)
{
  // convert 8 int2b_t to 8 int8b_t -> 2 int32
  uint *i8s = reinterpret_cast<uint *>(_i8s);

  // i2s = {e7,e6,e5,e4,e3,e2,e1,e0}
  // also require interleave {e7,e3,e6,e2,e5,e1,e4,e0}
  uint const i2s = *_i2s;

  // First, we extract the i4s and construct an intermediate fp16 number.
  static constexpr uint immLut = (0xf0 & 0xcc) | 0xaa;     // 0b11101010
  static constexpr uint BOTTOM_MASK = 0x03030303;          // 0xf -> 0b11 select 0,3
  static constexpr uint I4s_TO_I8s_MAGIC_NUM = 0x00000000; // 1024

#pragma unroll
  for (int i = 0; i < (N / 2); i++)
  {
    asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                 : "=r"(i8s[i])
                 : "r"(i2s >> (2 * i)), "n"(BOTTOM_MASK), "n"(I4s_TO_I8s_MAGIC_NUM), "n"(immLut));
  }
}
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 610)
#include <sm_61_intrinsics.h>


#if defined(__CUDACC_RTC__)
#define __SM_61_INTRINSICS_DECL__ __device__
#else /* !__CUDACC_RTC__ */
#define __SM_61_INTRINSICS_DECL__ static __device__ __inline__
#endif /* __CUDACC_RTC__ */

#ifndef __CUDA_ARCH__
#define __DEF_IF_HOST { }
#else  /* !__CUDA_ARCH__ */
#define __DEF_IF_HOST ;
#endif /* __CUDA_ARCH__ */

__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) __DEF_IF_HOST
__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) __DEF_IF_HOST

#undef __DEF_IF_HOST

#if !defined(__CUDACC_RTC__) && defined(__CUDA_ARCH__)
__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) {
    int ret;
    asm volatile ("dp4a.u32.s32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}

__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) {
    int ret;
    asm volatile ("dp4a.s32.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}
#endif /* !__CUDACC_RTC__ && defined(__CUDA_ARCH__) */

#undef __SM_61_INTRINSICS_DECL__

#endif
__forceinline__ __device__ unsigned int
cast_smem_ptr_to_int(const void* const smem_ptr)
{
  unsigned int smem_int;
  asm volatile ("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; cvt.u32.u64 %0, smem_int; }"
    : "=r"(smem_int) : "l"(smem_ptr));
  return smem_int;
}

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif

#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ == 800) 
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 1
#else
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 0
#endif
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C);
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C) {

        const int MAX_BLOCK_N = 10;
        const auto baseBlockIdx = blockIdx.x + gridDim.x *blockIdx.y;
        const auto totalPanel = (gridDim.x * gridDim.y +MAX_BLOCK_N * gridDim.x - 1) / (MAX_BLOCK_N * gridDim.x);
        const auto totalBlock = gridDim.x * gridDim.y;
        const auto panelIdx = baseBlockIdx / (MAX_BLOCK_N *gridDim.x);
        const auto strideLd = panelIdx + 1 < totalPanel ?MAX_BLOCK_N : (totalBlock - panelIdx * (MAX_BLOCK_N *gridDim.x)) / gridDim.x;
        const auto bx = (panelIdx & 1) ? gridDim.x -(baseBlockIdx - panelIdx * MAX_BLOCK_N * gridDim.x) /strideLd - 1 : (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) / strideLd;
        const auto by = (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) % strideLd + panelIdx * MAX_BLOCK_N;
        const auto bz = blockIdx.z;
        const dim3 blockIdx(bx, by, bz);
      __shared__ uchar buf_shmem[53248];
  int C_reindex_shared_warp[16];
  int B_local[1];
  int4 B_reindex_reindex_local[1];
  signed char A_reindex_shared_warp[32];
  signed char B_reindex_reindex_shared_warp[16];
  int B_local_1[1];
  int4 B_reindex_reindex_local_1[1];
  signed char A_reindex_shared_warp_1[32];
  signed char B_reindex_reindex_shared_warp_1[16];
  for (int var = 0; var < 1; ++var) {
    for (int ax1_0_3_init = 0; ax1_0_3_init < 2; ++ax1_0_3_init) {
      for (int i = 0; i < 8; ++i) {
C_reindex_shared_warp[(ax1_0_3_init * 8) + i] = 0.0;}
;
    }
    #pragma unroll
    for (int ax0_ax1_ax2_fused_2 = 0; ax0_ax1_ax2_fused_2 < 8; ++ax0_ax1_ax2_fused_2) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 4) * 256)) + (((((int)threadIdx.x) & 15) ^ ((ax0_ax1_ax2_fused_2 * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 20480))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 4) * 256)) + (((((int)threadIdx.x) & 15) ^ ((ax0_ax1_ax2_fused_2 * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 20480)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((int)blockIdx.y) * 1048576) + (((int)threadIdx.y) * 524288)) + (((int)threadIdx.z) * 262144)) + (ax0_ax1_ax2_fused_2 * 32768)) + ((((int)threadIdx.x) >> 4) * 16384)) + ((((int)threadIdx.x) & 15) * 16)))), "n"(16)
    );
  }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_fused_0 = 0; ax0_ax1_ax2_ax3_fused_0 < 1; ++ax0_ax1_ax2_ax3_fused_0) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((int)threadIdx.z) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((int)threadIdx.z) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + (((((((int)blockIdx.z) * 1048576) + (((int)blockIdx.x) * 131072)) + (((int)threadIdx.z) * 65536)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))), "n"(16)
    );
  }
    }
__asm__ __volatile__("cp.async.commit_group;");

    for (int ax3_0_0 = 0; ax3_0_0 < 63; ++ax3_0_0) {
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_fused_2_1 = 0; ax0_ax1_ax2_fused_2_1 < 8; ++ax0_ax1_ax2_fused_2_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 4) * 256)) + (((((int)threadIdx.x) & 15) ^ ((ax0_ax1_ax2_fused_2_1 * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 20480))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 4) * 256)) + (((((int)threadIdx.x) & 15) ^ ((ax0_ax1_ax2_fused_2_1 * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 20480)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int)blockIdx.y) * 1048576) + (((int)threadIdx.y) * 524288)) + (((int)threadIdx.z) * 262144)) + (ax0_ax1_ax2_fused_2_1 * 32768)) + ((((int)threadIdx.x) >> 4) * 16384)) + (ax3_0_0 * 256)) + ((((int)threadIdx.x) & 15) * 16)) + 256))), "n"(16)
    );
  }
      }
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_fused_0_1 = 0; ax0_ax1_ax2_ax3_fused_0_1 < 1; ++ax0_ax1_ax2_ax3_fused_0_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((ax3_0_0 + 1) & 1) * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((ax3_0_0 + 1) & 1) * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + (((((((((int)blockIdx.z) * 1048576) + (((int)blockIdx.x) * 131072)) + (((int)threadIdx.z) * 65536)) + (ax3_0_0 * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)) + 1024))), "n"(16)
    );
  }
      }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 1;");

      __syncthreads();
      for (int ax1_ax2_ax3_ax4_0_fused_0 = 0; ax1_ax2_ax3_ax4_0_fused_0 < 4; ++ax1_ax2_ax3_ax4_0_fused_0) {
        B_local[0] = *(int*)(((signed char*)buf_shmem) + ((((((ax3_0_0 & 1) * 2048) + (ax1_ax2_ax3_ax4_0_fused_0 * 512)) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)));
        decode_i2s_to_i8s(B_local, B_reindex_reindex_local, 16);
        *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 4096)) = B_reindex_reindex_local[0];
      }
      __syncthreads();
      for (int ax3_0_1 = 0; ax3_0_1 < 8; ++ax3_0_1) {
        for (int ax1_0 = 0; ax1_0 < 2; ++ax1_0) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (ax1_0 * 4096)) + ((((int)threadIdx.x) & 15) * 256)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 20480)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (ax1_0 * 4096)) + ((((int)threadIdx.x) & 15) * 256)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 20480)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[3])
      : "r"(addr)
    );
  }
        }

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((int)threadIdx.z) * 4096) + (ax3_0_1 * 512)) + 4096)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((int)threadIdx.z) * 4096) + (ax3_0_1 * 512)) + 4096)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[3])
      : "r"(addr)
    );
  }
        for (int ax1_0_3 = 0; ax1_0_3 < 2; ++ax1_0_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[0]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[1]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[2]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[0]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[2]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 8))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 8))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[3]));
  }
        }
      }
    }
__asm__ __volatile__("cp.async.wait_group 0;");

    __syncthreads();
    for (int ax1_ax2_ax3_ax4_0_fused_0_1 = 0; ax1_ax2_ax3_ax4_0_fused_0_1 < 4; ++ax1_ax2_ax3_ax4_0_fused_0_1) {
      B_local_1[0] = *(int*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 512) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)) + 2048));
      decode_i2s_to_i8s(B_local_1, B_reindex_reindex_local_1, 16);
      *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 4096)) = B_reindex_reindex_local_1[0];
    }
    __syncthreads();
    for (int ax3_0_1_1 = 0; ax3_0_1_1 < 8; ++ax3_0_1_1) {
      for (int ax1_0_1 = 0; ax1_0_1 < 2; ++ax1_0_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 8192) + (ax1_0_1 * 4096)) + ((((int)threadIdx.x) & 15) * 256)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 36864)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 8192) + (ax1_0_1 * 4096)) + ((((int)threadIdx.x) & 15) * 256)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 36864)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((int)threadIdx.z) * 4096) + (ax3_0_1_1 * 512)) + 4096)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((int)threadIdx.z) * 4096) + (ax3_0_1_1 * 512)) + 4096)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[3])
      : "r"(addr)
    );
  }
      for (int ax1_0_3_1 = 0; ax1_0_3_1 < 2; ++ax1_0_3_1) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[0]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[1]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[2]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[0]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[2]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 8))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 8))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[3]));
  }
      }
    }
    for (int ax0 = 0; ax0 < 2; ++ax0) {
      for (int local_id = 0; local_id < 8; ++local_id) {
(&(((int*)buf_shmem)[((((((int)threadIdx.y) * 1024) + (ax0 * 512)) + (((int)threadIdx.z) * 256)) + 3072)]))[((((((local_id % 4) / 2) * 8) + (threadIdx.x / 4)) * 16) + ((((local_id / 4) * 8) + ((threadIdx.x % 4) * 2)) + (local_id % 2)))] = C_reindex_shared_warp[(ax0 * 8) + local_id];
}
;
    }
    __syncthreads();
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_ax4_fused_0 = 0; ax0_ax1_ax2_ax3_ax4_fused_0 < 4; ++ax0_ax1_ax2_ax3_ax4_fused_0) {
      *(int4*)(C + ((((((((((int)blockIdx.y) * 1048576) + (((int)threadIdx.y) * 524288)) + (ax0_ax1_ax2_ax3_ax4_fused_0 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (((int)blockIdx.z) * 256)) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.z) * 16)) + ((((int)threadIdx.x) & 3) * 4))) = *(int4*)(((int*)buf_shmem) + ((((((((int)threadIdx.y) * 1024) + ((ax0_ax1_ax2_ax3_ax4_fused_0 >> 1) * 512)) + (((int)threadIdx.z) * 256)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 & 1) * 128)) + (((int)threadIdx.x) * 4)) + 3072));
    }
  }
}


Compilation error:
/tmp/tmpmlxj0z_y/tvm_kernels.cu(52): warning #177-D: function "__dp4a(int, unsigned int, int)" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/tmp/tmpmlxj0z_y/tvm_kernels.cu(46): warning #177-D: function "__dp4a(unsigned int, int, int)" was declared but never referenced

ptxas error   : Entry function 'default_function_kernel' uses too much shared data (0xd000 bytes, 0xc000 max)


[FastDlight] LocalBuilder: An exception occurred  Traceback (most recent call last):
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/t-leiwang/ladder_workspace/BitBLAS/python/bitblas/base/utils.py", line 201, in _build
    rt_mod = tvm.build(mod["main"], target=arch.target)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/driver/build_module.py", line 294, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 204, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 128, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: template <typename T1, typename T2>
__device__ void decode_i2s_to_i8s(T1 *_i2s, T2 *_i8s, const int N = 16)
{
  // convert 8 int2b_t to 8 int8b_t -> 2 int32
  uint *i8s = reinterpret_cast<uint *>(_i8s);

  // i2s = {e7,e6,e5,e4,e3,e2,e1,e0}
  // also require interleave {e7,e3,e6,e2,e5,e1,e4,e0}
  uint const i2s = *_i2s;

  // First, we extract the i4s and construct an intermediate fp16 number.
  static constexpr uint immLut = (0xf0 & 0xcc) | 0xaa;     // 0b11101010
  static constexpr uint BOTTOM_MASK = 0x03030303;          // 0xf -> 0b11 select 0,3
  static constexpr uint I4s_TO_I8s_MAGIC_NUM = 0x00000000; // 1024

#pragma unroll
  for (int i = 0; i < (N / 2); i++)
  {
    asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                 : "=r"(i8s[i])
                 : "r"(i2s >> (2 * i)), "n"(BOTTOM_MASK), "n"(I4s_TO_I8s_MAGIC_NUM), "n"(immLut));
  }
}
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 610)
#include <sm_61_intrinsics.h>


#if defined(__CUDACC_RTC__)
#define __SM_61_INTRINSICS_DECL__ __device__
#else /* !__CUDACC_RTC__ */
#define __SM_61_INTRINSICS_DECL__ static __device__ __inline__
#endif /* __CUDACC_RTC__ */

#ifndef __CUDA_ARCH__
#define __DEF_IF_HOST { }
#else  /* !__CUDA_ARCH__ */
#define __DEF_IF_HOST ;
#endif /* __CUDA_ARCH__ */

__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) __DEF_IF_HOST
__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) __DEF_IF_HOST

#undef __DEF_IF_HOST

#if !defined(__CUDACC_RTC__) && defined(__CUDA_ARCH__)
__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) {
    int ret;
    asm volatile ("dp4a.u32.s32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}

__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) {
    int ret;
    asm volatile ("dp4a.s32.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}
#endif /* !__CUDACC_RTC__ && defined(__CUDA_ARCH__) */

#undef __SM_61_INTRINSICS_DECL__

#endif
__forceinline__ __device__ unsigned int
cast_smem_ptr_to_int(const void* const smem_ptr)
{
  unsigned int smem_int;
  asm volatile ("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; cvt.u32.u64 %0, smem_int; }"
    : "=r"(smem_int) : "l"(smem_ptr));
  return smem_int;
}

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif

#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ == 800) 
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 1
#else
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 0
#endif
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C);
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C) {

        const int MAX_BLOCK_N = 10;
        const auto baseBlockIdx = blockIdx.x + gridDim.x *blockIdx.y;
        const auto totalPanel = (gridDim.x * gridDim.y +MAX_BLOCK_N * gridDim.x - 1) / (MAX_BLOCK_N * gridDim.x);
        const auto totalBlock = gridDim.x * gridDim.y;
        const auto panelIdx = baseBlockIdx / (MAX_BLOCK_N *gridDim.x);
        const auto strideLd = panelIdx + 1 < totalPanel ?MAX_BLOCK_N : (totalBlock - panelIdx * (MAX_BLOCK_N *gridDim.x)) / gridDim.x;
        const auto bx = (panelIdx & 1) ? gridDim.x -(baseBlockIdx - panelIdx * MAX_BLOCK_N * gridDim.x) /strideLd - 1 : (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) / strideLd;
        const auto by = (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) % strideLd + panelIdx * MAX_BLOCK_N;
        const auto bz = blockIdx.z;
        const dim3 blockIdx(bx, by, bz);
      __shared__ uchar buf_shmem[104448];
  int C_reindex_shared_warp[64];
  int B_local[1];
  int4 B_reindex_reindex_local[1];
  signed char A_reindex_shared_warp[128];
  signed char B_reindex_reindex_shared_warp[16];
  int B_local_1[1];
  int4 B_reindex_reindex_local_1[1];
  signed char A_reindex_shared_warp_1[128];
  signed char B_reindex_reindex_shared_warp_1[16];
  for (int var = 0; var < 1; ++var) {
    for (int ax1_0_3_init = 0; ax1_0_3_init < 8; ++ax1_0_3_init) {
      for (int i = 0; i < 8; ++i) {
C_reindex_shared_warp[(ax1_0_3_init * 8) + i] = 0.0;}
;
    }
    #pragma unroll
    for (int ax0_ax1_ax2_fused_2 = 0; ax0_ax1_ax2_fused_2 < 16; ++ax0_ax1_ax2_fused_2) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((int)threadIdx.y) * 16384) + (((int)threadIdx.z) * 8192)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((ax0_ax1_ax2_fused_2 & 3) * 4) + (((int)threadIdx.x) >> 3))) * 16)) + 38912))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((int)threadIdx.y) * 16384) + (((int)threadIdx.z) * 8192)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((ax0_ax1_ax2_fused_2 & 3) * 4) + (((int)threadIdx.x) >> 3))) * 16)) + 38912)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((int)blockIdx.y) * 4194304) + (((int)threadIdx.y) * 2097152)) + (((int)threadIdx.z) * 1048576)) + (ax0_ax1_ax2_fused_2 * 65536)) + ((((int)threadIdx.x) >> 3) * 16384)) + ((((int)threadIdx.x) & 7) * 16)))), "n"(16)
    );
  }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_fused_0 = 0; ax0_ax1_ax2_ax3_fused_0 < 1; ++ax0_ax1_ax2_ax3_fused_0) {
      if (((int)threadIdx.z) < 1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((int)threadIdx.y) * 512) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((int)threadIdx.y) * 512) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + (((((((int)blockIdx.z) * 1048576) + (((int)blockIdx.x) * 131072)) + (((int)threadIdx.z) * 131072)) + (((int)threadIdx.y) * 65536)) + (((int)threadIdx.x) * 16)))), "n"(16)
    );
  }
      }
    }
__asm__ __volatile__("cp.async.commit_group;");

    for (int ax3_0_0 = 0; ax3_0_0 < 127; ++ax3_0_0) {
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_fused_2_1 = 0; ax0_ax1_ax2_fused_2_1 < 16; ++ax0_ax1_ax2_fused_2_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 32768) + (((int)threadIdx.y) * 16384)) + (((int)threadIdx.z) * 8192)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((ax0_ax1_ax2_fused_2_1 & 3) * 4) + (((int)threadIdx.x) >> 3))) * 16)) + 38912))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 32768) + (((int)threadIdx.y) * 16384)) + (((int)threadIdx.z) * 8192)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((ax0_ax1_ax2_fused_2_1 & 3) * 4) + (((int)threadIdx.x) >> 3))) * 16)) + 38912)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int)blockIdx.y) * 4194304) + (((int)threadIdx.y) * 2097152)) + (((int)threadIdx.z) * 1048576)) + (ax0_ax1_ax2_fused_2_1 * 65536)) + ((((int)threadIdx.x) >> 3) * 16384)) + (ax3_0_0 * 128)) + ((((int)threadIdx.x) & 7) * 16)) + 128))), "n"(16)
    );
  }
      }
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_fused_0_1 = 0; ax0_ax1_ax2_ax3_fused_0_1 < 1; ++ax0_ax1_ax2_ax3_fused_0_1) {
        if (((int)threadIdx.z) < 1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((ax3_0_0 + 1) & 1) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((ax3_0_0 + 1) & 1) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + (((((((((int)blockIdx.z) * 1048576) + (((int)blockIdx.x) * 131072)) + (((int)threadIdx.z) * 131072)) + (((int)threadIdx.y) * 65536)) + (ax3_0_0 * 512)) + (((int)threadIdx.x) * 16)) + 512))), "n"(16)
    );
  }
        }
      }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 1;");

      __syncthreads();
      for (int ax1_ax2_ax3_ax4_0_fused_0 = 0; ax1_ax2_ax3_ax4_0_fused_0 < 2; ++ax1_ax2_ax3_ax4_0_fused_0) {
        B_local[0] = *(int*)(((signed char*)buf_shmem) + ((((((ax3_0_0 & 1) * 1024) + (ax1_ax2_ax3_ax4_0_fused_0 * 512)) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)));
        decode_i2s_to_i8s(B_local, B_reindex_reindex_local, 16);
        *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 2048)) = B_reindex_reindex_local[0];
      }
      __syncthreads();
      for (int ax3_0_1 = 0; ax3_0_1 < 4; ++ax3_0_1) {
        for (int ax1_0 = 0; ax1_0 < 8; ++ax1_0) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 32768) + (((int)threadIdx.y) * 16384)) + (ax1_0 * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 38912)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 32768) + (((int)threadIdx.y) * 16384)) + (ax1_0 * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 38912)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[3])
      : "r"(addr)
    );
  }
        }

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((int)threadIdx.z) * 2048) + (ax3_0_1 * 512)) + 2048)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((int)threadIdx.z) * 2048) + (ax3_0_1 * 512)) + 2048)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[3])
      : "r"(addr)
    );
  }
        for (int ax1_0_3 = 0; ax1_0_3 < 8; ++ax1_0_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[0]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[1]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[2]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[0]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[2]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 8))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 8))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[3]));
  }
        }
      }
    }
__asm__ __volatile__("cp.async.wait_group 0;");

    __syncthreads();
    for (int ax1_ax2_ax3_ax4_0_fused_0_1 = 0; ax1_ax2_ax3_ax4_0_fused_0_1 < 2; ++ax1_ax2_ax3_ax4_0_fused_0_1) {
      B_local_1[0] = *(int*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 512) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)) + 1024));
      decode_i2s_to_i8s(B_local_1, B_reindex_reindex_local_1, 16);
      *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 2048)) = B_reindex_reindex_local_1[0];
    }
    __syncthreads();
    for (int ax3_0_1_1 = 0; ax3_0_1_1 < 4; ++ax3_0_1_1) {
      for (int ax1_0_1 = 0; ax1_0_1 < 8; ++ax1_0_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 16384) + (ax1_0_1 * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 71680)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 16384) + (ax1_0_1 * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 71680)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((int)threadIdx.z) * 2048) + (ax3_0_1_1 * 512)) + 2048)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((int)threadIdx.z) * 2048) + (ax3_0_1_1 * 512)) + 2048)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[3])
      : "r"(addr)
    );
  }
      for (int ax1_0_3_1 = 0; ax1_0_3_1 < 8; ++ax1_0_3_1) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[0]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[1]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[2]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[0]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[2]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 8))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 8))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[3]));
  }
      }
    }
    for (int ax0 = 0; ax0 < 8; ++ax0) {
      for (int local_id = 0; local_id < 8; ++local_id) {
(&(((int*)buf_shmem)[((((((int)threadIdx.y) * 4096) + (ax0 * 512)) + (((int)threadIdx.z) * 256)) + 1536)]))[((((((local_id % 4) / 2) * 8) + (threadIdx.x / 4)) * 16) + ((((local_id / 4) * 8) + ((threadIdx.x % 4) * 2)) + (local_id % 2)))] = C_reindex_shared_warp[(ax0 * 8) + local_id];
}
;
    }
    __syncthreads();
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_ax4_fused_0 = 0; ax0_ax1_ax2_ax3_ax4_fused_0 < 16; ++ax0_ax1_ax2_ax3_ax4_fused_0) {
      *(int4*)(C + ((((((((((int)blockIdx.y) * 4194304) + (((int)threadIdx.y) * 2097152)) + (ax0_ax1_ax2_ax3_ax4_fused_0 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (((int)blockIdx.z) * 256)) + (((int)blockIdx.x) * 32)) + (((int)threadIdx.z) * 16)) + ((((int)threadIdx.x) & 3) * 4))) = *(int4*)(((int*)buf_shmem) + ((((((((int)threadIdx.y) * 4096) + ((ax0_ax1_ax2_ax3_ax4_fused_0 >> 1) * 512)) + (((int)threadIdx.z) * 256)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 & 1) * 128)) + (((int)threadIdx.x) * 4)) + 1536));
    }
  }
}


Compilation error:
/tmp/tmp_slfyv4d/tvm_kernels.cu(52): warning #177-D: function "__dp4a(int, unsigned int, int)" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/tmp/tmp_slfyv4d/tvm_kernels.cu(46): warning #177-D: function "__dp4a(unsigned int, int, int)" was declared but never referenced

ptxas error   : Entry function 'default_function_kernel' uses too much shared data (0x19800 bytes, 0xc000 max)


[FastDlight] LocalBuilder: An exception occurred  Traceback (most recent call last):
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/t-leiwang/ladder_workspace/BitBLAS/python/bitblas/base/utils.py", line 201, in _build
    rt_mod = tvm.build(mod["main"], target=arch.target)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/driver/build_module.py", line 294, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 204, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 128, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: template <typename T1, typename T2>
__device__ void decode_i2s_to_i8s(T1 *_i2s, T2 *_i8s, const int N = 16)
{
  // convert 8 int2b_t to 8 int8b_t -> 2 int32
  uint *i8s = reinterpret_cast<uint *>(_i8s);

  // i2s = {e7,e6,e5,e4,e3,e2,e1,e0}
  // also require interleave {e7,e3,e6,e2,e5,e1,e4,e0}
  uint const i2s = *_i2s;

  // First, we extract the i4s and construct an intermediate fp16 number.
  static constexpr uint immLut = (0xf0 & 0xcc) | 0xaa;     // 0b11101010
  static constexpr uint BOTTOM_MASK = 0x03030303;          // 0xf -> 0b11 select 0,3
  static constexpr uint I4s_TO_I8s_MAGIC_NUM = 0x00000000; // 1024

#pragma unroll
  for (int i = 0; i < (N / 2); i++)
  {
    asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                 : "=r"(i8s[i])
                 : "r"(i2s >> (2 * i)), "n"(BOTTOM_MASK), "n"(I4s_TO_I8s_MAGIC_NUM), "n"(immLut));
  }
}
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 610)
#include <sm_61_intrinsics.h>


#if defined(__CUDACC_RTC__)
#define __SM_61_INTRINSICS_DECL__ __device__
#else /* !__CUDACC_RTC__ */
#define __SM_61_INTRINSICS_DECL__ static __device__ __inline__
#endif /* __CUDACC_RTC__ */

#ifndef __CUDA_ARCH__
#define __DEF_IF_HOST { }
#else  /* !__CUDA_ARCH__ */
#define __DEF_IF_HOST ;
#endif /* __CUDA_ARCH__ */

__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) __DEF_IF_HOST
__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) __DEF_IF_HOST

#undef __DEF_IF_HOST

#if !defined(__CUDACC_RTC__) && defined(__CUDA_ARCH__)
__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) {
    int ret;
    asm volatile ("dp4a.u32.s32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}

__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) {
    int ret;
    asm volatile ("dp4a.s32.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}
#endif /* !__CUDACC_RTC__ && defined(__CUDA_ARCH__) */

#undef __SM_61_INTRINSICS_DECL__

#endif
__forceinline__ __device__ unsigned int
cast_smem_ptr_to_int(const void* const smem_ptr)
{
  unsigned int smem_int;
  asm volatile ("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; cvt.u32.u64 %0, smem_int; }"
    : "=r"(smem_int) : "l"(smem_ptr));
  return smem_int;
}

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif

#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ == 800) 
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 1
#else
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 0
#endif
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C);
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C) {

        const int MAX_BLOCK_N = 10;
        const auto baseBlockIdx = blockIdx.x + gridDim.x *blockIdx.y;
        const auto totalPanel = (gridDim.x * gridDim.y +MAX_BLOCK_N * gridDim.x - 1) / (MAX_BLOCK_N * gridDim.x);
        const auto totalBlock = gridDim.x * gridDim.y;
        const auto panelIdx = baseBlockIdx / (MAX_BLOCK_N *gridDim.x);
        const auto strideLd = panelIdx + 1 < totalPanel ?MAX_BLOCK_N : (totalBlock - panelIdx * (MAX_BLOCK_N *gridDim.x)) / gridDim.x;
        const auto bx = (panelIdx & 1) ? gridDim.x -(baseBlockIdx - panelIdx * MAX_BLOCK_N * gridDim.x) /strideLd - 1 : (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) / strideLd;
        const auto by = (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) % strideLd + panelIdx * MAX_BLOCK_N;
        const auto bz = blockIdx.z;
        const dim3 blockIdx(bx, by, bz);
      __shared__ uchar buf_shmem[118784];
  int C_reindex_shared_warp[256];
  int B_local[1];
  int4 B_reindex_reindex_local[1];
  signed char A_reindex_shared_warp[128];
  signed char B_reindex_reindex_shared_warp[64];
  int B_local_1[1];
  int4 B_reindex_reindex_local_1[1];
  signed char A_reindex_shared_warp_1[128];
  signed char B_reindex_reindex_shared_warp_1[64];
  for (int var = 0; var < 1; ++var) {
    for (int ax1_0_3_init = 0; ax1_0_3_init < 8; ++ax1_0_3_init) {
      for (int ax2_0_3_init = 0; ax2_0_3_init < 4; ++ax2_0_3_init) {
        for (int i = 0; i < 8; ++i) {
C_reindex_shared_warp[((ax1_0_3_init * 32) + (ax2_0_3_init * 8)) + i] = 0.0;}
;
      }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_fused_2 = 0; ax0_ax1_ax2_fused_2 < 8; ++ax0_ax1_ax2_fused_2) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 12288))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 12288)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((int)blockIdx.y) * 4194304) + (((int)threadIdx.y) * 2097152)) + (((int)threadIdx.z) * 1048576)) + (ax0_ax1_ax2_fused_2 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + ((((int)threadIdx.x) & 3) * 16)))), "n"(16)
    );
  }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_fused_0 = 0; ax0_ax1_ax2_ax3_fused_0 < 1; ++ax0_ax1_ax2_ax3_fused_0) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((int)threadIdx.z) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((int)threadIdx.z) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((int)blockIdx.z) * 4194304) + (((int)blockIdx.x) * 524288)) + (((int)threadIdx.z) * 262144)) + (((int)threadIdx.y) * 131072)) + ((((int)threadIdx.x) >> 4) * 65536)) + ((((int)threadIdx.x) & 15) * 16)))), "n"(16)
    );
  }
    }
__asm__ __volatile__("cp.async.commit_group;");

    for (int ax3_0_0 = 0; ax3_0_0 < 255; ++ax3_0_0) {
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_fused_2_1 = 0; ax0_ax1_ax2_fused_2_1 < 8; ++ax0_ax1_ax2_fused_2_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2_1 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 12288))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (((int)threadIdx.z) * 4096)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2_1 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 12288)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int)blockIdx.y) * 4194304) + (((int)threadIdx.y) * 2097152)) + (((int)threadIdx.z) * 1048576)) + (ax0_ax1_ax2_fused_2_1 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (ax3_0_0 * 64)) + ((((int)threadIdx.x) & 3) * 16)) + 64))), "n"(16)
    );
  }
      }
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_fused_0_1 = 0; ax0_ax1_ax2_ax3_fused_0_1 < 1; ++ax0_ax1_ax2_ax3_fused_0_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((ax3_0_0 + 1) & 1) * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((ax3_0_0 + 1) & 1) * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int)blockIdx.z) * 4194304) + (((int)blockIdx.x) * 524288)) + (((int)threadIdx.z) * 262144)) + (((int)threadIdx.y) * 131072)) + ((((int)threadIdx.x) >> 4) * 65536)) + (ax3_0_0 * 256)) + ((((int)threadIdx.x) & 15) * 16)) + 256))), "n"(16)
    );
  }
      }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 1;");

      __syncthreads();
      for (int ax1_ax2_ax3_ax4_0_fused_0 = 0; ax1_ax2_ax3_ax4_0_fused_0 < 4; ++ax1_ax2_ax3_ax4_0_fused_0) {
        B_local[0] = *(int*)(((signed char*)buf_shmem) + ((((((ax3_0_0 & 1) * 2048) + (ax1_ax2_ax3_ax4_0_fused_0 * 512)) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)));
        decode_i2s_to_i8s(B_local, B_reindex_reindex_local, 16);
        *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 4096)) = B_reindex_reindex_local[0];
      }
      __syncthreads();
      for (int ax3_0_1 = 0; ax3_0_1 < 2; ++ax3_0_1) {
        for (int ax1_0 = 0; ax1_0 < 8; ++ax1_0) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (ax1_0 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 12288)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 16384) + (((int)threadIdx.y) * 8192)) + (ax1_0 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 12288)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax1 = 0; ax1 < 4; ++ax1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 4096) + (ax1 * 1024)) + (ax3_0_1 * 512)) + 4096)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 4096) + (ax1 * 1024)) + (ax3_0_1 * 512)) + 4096)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax1_0_3 = 0; ax1_0_3 < 8; ++ax1_0_3) {
          for (int ax2_0_3 = 0; ax2_0_3 < 4; ++ax2_0_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[3]));
  }
          }
        }
      }
    }
__asm__ __volatile__("cp.async.wait_group 0;");

    __syncthreads();
    for (int ax1_ax2_ax3_ax4_0_fused_0_1 = 0; ax1_ax2_ax3_ax4_0_fused_0_1 < 4; ++ax1_ax2_ax3_ax4_0_fused_0_1) {
      B_local_1[0] = *(int*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 512) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)) + 2048));
      decode_i2s_to_i8s(B_local_1, B_reindex_reindex_local_1, 16);
      *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 4096)) = B_reindex_reindex_local_1[0];
    }
    __syncthreads();
    for (int ax3_0_1_1 = 0; ax3_0_1_1 < 2; ++ax3_0_1_1) {
      for (int ax1_0_1 = 0; ax1_0_1 < 8; ++ax1_0_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 8192) + (ax1_0_1 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 28672)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 8192) + (ax1_0_1 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 28672)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax1_1 = 0; ax1_1 < 4; ++ax1_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 4096) + (ax1_1 * 1024)) + (ax3_0_1_1 * 512)) + 4096)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 4096) + (ax1_1 * 1024)) + (ax3_0_1_1 * 512)) + 4096)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax1_0_3_1 = 0; ax1_0_3_1 < 8; ++ax1_0_3_1) {
        for (int ax2_0_3_1 = 0; ax2_0_3_1 < 4; ++ax2_0_3_1) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[3]));
  }
        }
      }
    }
    for (int ax0 = 0; ax0 < 8; ++ax0) {
      __syncthreads();
      for (int ax1_2 = 0; ax1_2 < 4; ++ax1_2) {
        for (int local_id = 0; local_id < 8; ++local_id) {
(&(((int*)buf_shmem)[((((((int)threadIdx.y) * 16384) + (((int)threadIdx.z) * 1024)) + (ax1_2 * 256)) + 11264)]))[((((((local_id % 4) / 2) * 8) + (threadIdx.x / 4)) * 16) + ((((local_id / 4) * 8) + ((threadIdx.x % 4) * 2)) + (local_id % 2)))] = C_reindex_shared_warp[((ax0 * 32) + (ax1_2 * 8)) + local_id];
}
;
      }
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_ax4_fused_0 = 0; ax0_ax1_ax2_ax3_ax4_fused_0 < 8; ++ax0_ax1_ax2_ax3_ax4_fused_0) {
        *(int4*)(C + ((((((((((((int)blockIdx.y) * 4194304) + (((int)threadIdx.y) * 2097152)) + (ax0 * 262144)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 & 1) * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (((int)blockIdx.z) * 1024)) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.z) * 64)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4))) = *(int4*)(((int*)buf_shmem) + (((((((int)threadIdx.y) * 16384) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_ax3_ax4_fused_0 * 128)) + (((int)threadIdx.x) * 4)) + 11264));
      }
    }
  }
}


Compilation error:
/tmp/tmp8_6jny4x/tvm_kernels.cu(52): warning #177-D: function "__dp4a(int, unsigned int, int)" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/tmp/tmp8_6jny4x/tvm_kernels.cu(46): warning #177-D: function "__dp4a(unsigned int, int, int)" was declared but never referenced

ptxas error   : Entry function 'default_function_kernel' uses too much shared data (0x1d000 bytes, 0xc000 max)


[FastDlight] LocalBuilder: An exception occurred  Traceback (most recent call last):
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/t-leiwang/ladder_workspace/BitBLAS/python/bitblas/base/utils.py", line 201, in _build
    rt_mod = tvm.build(mod["main"], target=arch.target)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/driver/build_module.py", line 294, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 204, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 128, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: template <typename T1, typename T2>
__device__ void decode_i2s_to_i8s(T1 *_i2s, T2 *_i8s, const int N = 16)
{
  // convert 8 int2b_t to 8 int8b_t -> 2 int32
  uint *i8s = reinterpret_cast<uint *>(_i8s);

  // i2s = {e7,e6,e5,e4,e3,e2,e1,e0}
  // also require interleave {e7,e3,e6,e2,e5,e1,e4,e0}
  uint const i2s = *_i2s;

  // First, we extract the i4s and construct an intermediate fp16 number.
  static constexpr uint immLut = (0xf0 & 0xcc) | 0xaa;     // 0b11101010
  static constexpr uint BOTTOM_MASK = 0x03030303;          // 0xf -> 0b11 select 0,3
  static constexpr uint I4s_TO_I8s_MAGIC_NUM = 0x00000000; // 1024

#pragma unroll
  for (int i = 0; i < (N / 2); i++)
  {
    asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                 : "=r"(i8s[i])
                 : "r"(i2s >> (2 * i)), "n"(BOTTOM_MASK), "n"(I4s_TO_I8s_MAGIC_NUM), "n"(immLut));
  }
}
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 610)
#include <sm_61_intrinsics.h>


#if defined(__CUDACC_RTC__)
#define __SM_61_INTRINSICS_DECL__ __device__
#else /* !__CUDACC_RTC__ */
#define __SM_61_INTRINSICS_DECL__ static __device__ __inline__
#endif /* __CUDACC_RTC__ */

#ifndef __CUDA_ARCH__
#define __DEF_IF_HOST { }
#else  /* !__CUDA_ARCH__ */
#define __DEF_IF_HOST ;
#endif /* __CUDA_ARCH__ */

__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) __DEF_IF_HOST
__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) __DEF_IF_HOST

#undef __DEF_IF_HOST

#if !defined(__CUDACC_RTC__) && defined(__CUDA_ARCH__)
__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) {
    int ret;
    asm volatile ("dp4a.u32.s32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}

__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) {
    int ret;
    asm volatile ("dp4a.s32.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}
#endif /* !__CUDACC_RTC__ && defined(__CUDA_ARCH__) */

#undef __SM_61_INTRINSICS_DECL__

#endif
__forceinline__ __device__ unsigned int
cast_smem_ptr_to_int(const void* const smem_ptr)
{
  unsigned int smem_int;
  asm volatile ("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; cvt.u32.u64 %0, smem_int; }"
    : "=r"(smem_int) : "l"(smem_ptr));
  return smem_int;
}

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif

#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ == 800) 
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 1
#else
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 0
#endif
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C);
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C) {

        const int MAX_BLOCK_N = 10;
        const auto baseBlockIdx = blockIdx.x + gridDim.x *blockIdx.y;
        const auto totalPanel = (gridDim.x * gridDim.y +MAX_BLOCK_N * gridDim.x - 1) / (MAX_BLOCK_N * gridDim.x);
        const auto totalBlock = gridDim.x * gridDim.y;
        const auto panelIdx = baseBlockIdx / (MAX_BLOCK_N *gridDim.x);
        const auto strideLd = panelIdx + 1 < totalPanel ?MAX_BLOCK_N : (totalBlock - panelIdx * (MAX_BLOCK_N *gridDim.x)) / gridDim.x;
        const auto bx = (panelIdx & 1) ? gridDim.x -(baseBlockIdx - panelIdx * MAX_BLOCK_N * gridDim.x) /strideLd - 1 : (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) / strideLd;
        const auto by = (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) % strideLd + panelIdx * MAX_BLOCK_N;
        const auto bz = blockIdx.z;
        const dim3 blockIdx(bx, by, bz);
      __shared__ uchar buf_shmem[81920];
  int C_reindex_shared_warp[128];
  int B_local[1];
  int4 B_reindex_reindex_local[1];
  signed char A_reindex_shared_warp[32];
  signed char B_reindex_reindex_shared_warp[128];
  int B_local_1[1];
  int4 B_reindex_reindex_local_1[1];
  signed char A_reindex_shared_warp_1[32];
  signed char B_reindex_reindex_shared_warp_1[128];
  for (int var = 0; var < 1; ++var) {
    for (int ax1_0_3_init = 0; ax1_0_3_init < 2; ++ax1_0_3_init) {
      for (int ax2_0_3_init = 0; ax2_0_3_init < 8; ++ax2_0_3_init) {
        for (int i = 0; i < 8; ++i) {
C_reindex_shared_warp[((ax1_0_3_init * 64) + (ax2_0_3_init * 8)) + i] = 0.0;}
;
      }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_fused_2 = 0; ax0_ax1_ax2_fused_2 < 2; ++ax0_ax1_ax2_fused_2) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((((int)threadIdx.y) * 2048) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ ((ax0_ax1_ax2_fused_2 * 2) + (((int)threadIdx.x) >> 4))) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((((int)threadIdx.y) * 2048) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ ((ax0_ax1_ax2_fused_2 * 2) + (((int)threadIdx.x) >> 4))) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((int)blockIdx.y) * 1048576) + (((int)threadIdx.y) * 524288)) + (((int)threadIdx.z) * 262144)) + (ax0_ax1_ax2_fused_2 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + ((((int)threadIdx.x) & 3) * 16)))), "n"(16)
    );
  }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_fused_0 = 0; ax0_ax1_ax2_ax3_fused_0 < 2; ++ax0_ax1_ax2_ax3_fused_0) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((ax0_ax1_ax2_ax3_fused_0 * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)) + 8192))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((ax0_ax1_ax2_ax3_fused_0 * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)) + 8192)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + (((((((((int)blockIdx.z) * 8388608) + (((int)blockIdx.x) * 1048576)) + (ax0_ax1_ax2_ax3_fused_0 * 524288)) + (((int)threadIdx.z) * 262144)) + (((int)threadIdx.y) * 131072)) + ((((int)threadIdx.x) >> 4) * 65536)) + ((((int)threadIdx.x) & 15) * 16)))), "n"(16)
    );
  }
    }
__asm__ __volatile__("cp.async.commit_group;");

    for (int ax3_0_0 = 0; ax3_0_0 < 255; ++ax3_0_0) {
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_fused_2_1 = 0; ax0_ax1_ax2_fused_2_1 < 2; ++ax0_ax1_ax2_fused_2_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((ax3_0_0 + 1) & 1) * 4096) + (((int)threadIdx.y) * 2048)) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ ((ax0_ax1_ax2_fused_2_1 * 2) + (((int)threadIdx.x) >> 4))) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((ax3_0_0 + 1) & 1) * 4096) + (((int)threadIdx.y) * 2048)) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ ((ax0_ax1_ax2_fused_2_1 * 2) + (((int)threadIdx.x) >> 4))) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int)blockIdx.y) * 1048576) + (((int)threadIdx.y) * 524288)) + (((int)threadIdx.z) * 262144)) + (ax0_ax1_ax2_fused_2_1 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (ax3_0_0 * 64)) + ((((int)threadIdx.x) & 3) * 16)) + 64))), "n"(16)
    );
  }
      }
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_fused_0_1 = 0; ax0_ax1_ax2_ax3_fused_0_1 < 2; ++ax0_ax1_ax2_ax3_fused_0_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((ax3_0_0 + 1) & 1) * 4096) + (ax0_ax1_ax2_ax3_fused_0_1 * 2048)) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)) + 8192))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((ax3_0_0 + 1) & 1) * 4096) + (ax0_ax1_ax2_ax3_fused_0_1 * 2048)) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)) + 8192)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + (((((((((((int)blockIdx.z) * 8388608) + (((int)blockIdx.x) * 1048576)) + (ax0_ax1_ax2_ax3_fused_0_1 * 524288)) + (((int)threadIdx.z) * 262144)) + (((int)threadIdx.y) * 131072)) + ((((int)threadIdx.x) >> 4) * 65536)) + (ax3_0_0 * 256)) + ((((int)threadIdx.x) & 15) * 16)) + 256))), "n"(16)
    );
  }
      }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 1;");

      __syncthreads();
      for (int ax1_ax2_ax3_ax4_0_fused_0 = 0; ax1_ax2_ax3_ax4_0_fused_0 < 8; ++ax1_ax2_ax3_ax4_0_fused_0) {
        B_local[0] = *(int*)(((signed char*)buf_shmem) + (((((((ax3_0_0 & 1) * 4096) + (ax1_ax2_ax3_ax4_0_fused_0 * 512)) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)) + 8192));
        decode_i2s_to_i8s(B_local, B_reindex_reindex_local, 16);
        *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 16384)) = B_reindex_reindex_local[0];
      }
      __syncthreads();
      for (int ax3_0_1 = 0; ax3_0_1 < 2; ++ax3_0_1) {
        for (int ax1_0 = 0; ax1_0 < 2; ++ax1_0) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((ax3_0_0 & 1) * 4096) + (((int)threadIdx.y) * 2048)) + (ax1_0 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16))])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((ax3_0_0 & 1) * 4096) + (((int)threadIdx.y) * 2048)) + (ax1_0 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16))])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax1 = 0; ax1 < 8; ++ax1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 8192) + (ax1 * 1024)) + (ax3_0_1 * 512)) + 16384)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 8192) + (ax1 * 1024)) + (ax3_0_1 * 512)) + 16384)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax1_0_3 = 0; ax1_0_3 < 2; ++ax1_0_3) {
          for (int ax2_0_3 = 0; ax2_0_3 < 8; ++ax2_0_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[3]));
  }
          }
        }
      }
    }
__asm__ __volatile__("cp.async.wait_group 0;");

    __syncthreads();
    for (int ax1_ax2_ax3_ax4_0_fused_0_1 = 0; ax1_ax2_ax3_ax4_0_fused_0_1 < 8; ++ax1_ax2_ax3_ax4_0_fused_0_1) {
      B_local_1[0] = *(int*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 512) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)) + 12288));
      decode_i2s_to_i8s(B_local_1, B_reindex_reindex_local_1, 16);
      *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 16384)) = B_reindex_reindex_local_1[0];
    }
    __syncthreads();
    for (int ax3_0_1_1 = 0; ax3_0_1_1 < 2; ++ax3_0_1_1) {
      for (int ax1_0_1 = 0; ax1_0_1 < 2; ++ax1_0_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 2048) + (ax1_0_1 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 4096)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 2048) + (ax1_0_1 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 4096)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax1_1 = 0; ax1_1 < 8; ++ax1_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 8192) + (ax1_1 * 1024)) + (ax3_0_1_1 * 512)) + 16384)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 8192) + (ax1_1 * 1024)) + (ax3_0_1_1 * 512)) + 16384)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax1_0_3_1 = 0; ax1_0_3_1 < 2; ++ax1_0_3_1) {
        for (int ax2_0_3_1 = 0; ax2_0_3_1 < 8; ++ax2_0_3_1) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[3]));
  }
        }
      }
    }
    for (int ax0 = 0; ax0 < 2; ++ax0) {
      __syncthreads();
      for (int ax1_2 = 0; ax1_2 < 8; ++ax1_2) {
        for (int local_id = 0; local_id < 8; ++local_id) {
(&(((int*)buf_shmem)[((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 2048)) + (ax1_2 * 256)) + 8192)]))[((((((local_id % 4) / 2) * 8) + (threadIdx.x / 4)) * 16) + ((((local_id / 4) * 8) + ((threadIdx.x % 4) * 2)) + (local_id % 2)))] = C_reindex_shared_warp[((ax0 * 64) + (ax1_2 * 8)) + local_id];
}
;
      }
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_ax4_fused_0 = 0; ax0_ax1_ax2_ax3_ax4_fused_0 < 16; ++ax0_ax1_ax2_ax3_ax4_fused_0) {
        *(int4*)(C + ((((((((((((int)blockIdx.y) * 1048576) + (((int)threadIdx.y) * 524288)) + (ax0 * 262144)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 & 1) * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (((int)blockIdx.z) * 2048)) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.z) * 128)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4))) = *(int4*)(((int*)buf_shmem) + (((((((int)threadIdx.y) * 8192) + (((int)threadIdx.z) * 2048)) + (ax0_ax1_ax2_ax3_ax4_fused_0 * 128)) + (((int)threadIdx.x) * 4)) + 8192));
      }
    }
  }
}


Compilation error:
/tmp/tmpjic8osyc/tvm_kernels.cu(52): warning #177-D: function "__dp4a(int, unsigned int, int)" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/tmp/tmpjic8osyc/tvm_kernels.cu(46): warning #177-D: function "__dp4a(unsigned int, int, int)" was declared but never referenced

ptxas error   : Entry function 'default_function_kernel' uses too much shared data (0x14000 bytes, 0xc000 max)


[FastDlight] LocalBuilder: An exception occurred  Traceback (most recent call last):
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/t-leiwang/ladder_workspace/BitBLAS/python/bitblas/base/utils.py", line 201, in _build
    rt_mod = tvm.build(mod["main"], target=arch.target)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/driver/build_module.py", line 294, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 204, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 128, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: template <typename T1, typename T2>
__device__ void decode_i2s_to_i8s(T1 *_i2s, T2 *_i8s, const int N = 16)
{
  // convert 8 int2b_t to 8 int8b_t -> 2 int32
  uint *i8s = reinterpret_cast<uint *>(_i8s);

  // i2s = {e7,e6,e5,e4,e3,e2,e1,e0}
  // also require interleave {e7,e3,e6,e2,e5,e1,e4,e0}
  uint const i2s = *_i2s;

  // First, we extract the i4s and construct an intermediate fp16 number.
  static constexpr uint immLut = (0xf0 & 0xcc) | 0xaa;     // 0b11101010
  static constexpr uint BOTTOM_MASK = 0x03030303;          // 0xf -> 0b11 select 0,3
  static constexpr uint I4s_TO_I8s_MAGIC_NUM = 0x00000000; // 1024

#pragma unroll
  for (int i = 0; i < (N / 2); i++)
  {
    asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                 : "=r"(i8s[i])
                 : "r"(i2s >> (2 * i)), "n"(BOTTOM_MASK), "n"(I4s_TO_I8s_MAGIC_NUM), "n"(immLut));
  }
}
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 610)
#include <sm_61_intrinsics.h>


#if defined(__CUDACC_RTC__)
#define __SM_61_INTRINSICS_DECL__ __device__
#else /* !__CUDACC_RTC__ */
#define __SM_61_INTRINSICS_DECL__ static __device__ __inline__
#endif /* __CUDACC_RTC__ */

#ifndef __CUDA_ARCH__
#define __DEF_IF_HOST { }
#else  /* !__CUDA_ARCH__ */
#define __DEF_IF_HOST ;
#endif /* __CUDA_ARCH__ */

__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) __DEF_IF_HOST
__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) __DEF_IF_HOST

#undef __DEF_IF_HOST

#if !defined(__CUDACC_RTC__) && defined(__CUDA_ARCH__)
__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) {
    int ret;
    asm volatile ("dp4a.u32.s32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}

__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) {
    int ret;
    asm volatile ("dp4a.s32.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}
#endif /* !__CUDACC_RTC__ && defined(__CUDA_ARCH__) */

#undef __SM_61_INTRINSICS_DECL__

#endif
__forceinline__ __device__ unsigned int
cast_smem_ptr_to_int(const void* const smem_ptr)
{
  unsigned int smem_int;
  asm volatile ("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; cvt.u32.u64 %0, smem_int; }"
    : "=r"(smem_int) : "l"(smem_ptr));
  return smem_int;
}

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif

#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ == 800) 
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 1
#else
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 0
#endif
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C);
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C) {

        const int MAX_BLOCK_N = 10;
        const auto baseBlockIdx = blockIdx.x + gridDim.x *blockIdx.y;
        const auto totalPanel = (gridDim.x * gridDim.y +MAX_BLOCK_N * gridDim.x - 1) / (MAX_BLOCK_N * gridDim.x);
        const auto totalBlock = gridDim.x * gridDim.y;
        const auto panelIdx = baseBlockIdx / (MAX_BLOCK_N *gridDim.x);
        const auto strideLd = panelIdx + 1 < totalPanel ?MAX_BLOCK_N : (totalBlock - panelIdx * (MAX_BLOCK_N *gridDim.x)) / gridDim.x;
        const auto bx = (panelIdx & 1) ? gridDim.x -(baseBlockIdx - panelIdx * MAX_BLOCK_N * gridDim.x) /strideLd - 1 : (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) / strideLd;
        const auto by = (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) % strideLd + panelIdx * MAX_BLOCK_N;
        const auto bz = blockIdx.z;
        const dim3 blockIdx(bx, by, bz);
      __shared__ uchar buf_shmem[122880];
  int C_reindex_shared_warp[256];
  int B_local[1];
  int4 B_reindex_reindex_local[1];
  signed char A_reindex_shared_warp[64];
  signed char B_reindex_reindex_shared_warp[128];
  int B_local_1[1];
  int4 B_reindex_reindex_local_1[1];
  signed char A_reindex_shared_warp_1[64];
  signed char B_reindex_reindex_shared_warp_1[128];
  for (int var = 0; var < 1; ++var) {
    for (int ax1_0_3_init = 0; ax1_0_3_init < 4; ++ax1_0_3_init) {
      for (int ax2_0_3_init = 0; ax2_0_3_init < 8; ++ax2_0_3_init) {
        for (int i = 0; i < 8; ++i) {
C_reindex_shared_warp[((ax1_0_3_init * 64) + (ax2_0_3_init * 8)) + i] = 0.0;}
;
      }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_fused_2 = 0; ax0_ax1_ax2_fused_2 < 4; ++ax0_ax1_ax2_fused_2) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((int)threadIdx.y) * 4096) + (((int)threadIdx.z) * 2048)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 8192))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((int)threadIdx.y) * 4096) + (((int)threadIdx.z) * 2048)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 8192)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((int)blockIdx.y) * 2097152) + (((int)threadIdx.y) * 1048576)) + (((int)threadIdx.z) * 524288)) + (ax0_ax1_ax2_fused_2 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + ((((int)threadIdx.x) & 3) * 16)))), "n"(16)
    );
  }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_fused_0 = 0; ax0_ax1_ax2_ax3_fused_0 < 2; ++ax0_ax1_ax2_ax3_fused_0) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((ax0_ax1_ax2_ax3_fused_0 * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((ax0_ax1_ax2_ax3_fused_0 * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + (((((((((int)blockIdx.z) * 8388608) + (((int)blockIdx.x) * 1048576)) + (ax0_ax1_ax2_ax3_fused_0 * 524288)) + (((int)threadIdx.z) * 262144)) + (((int)threadIdx.y) * 131072)) + ((((int)threadIdx.x) >> 4) * 65536)) + ((((int)threadIdx.x) & 15) * 16)))), "n"(16)
    );
  }
    }
__asm__ __volatile__("cp.async.commit_group;");

    for (int ax3_0_0 = 0; ax3_0_0 < 255; ++ax3_0_0) {
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_fused_2_1 = 0; ax0_ax1_ax2_fused_2_1 < 4; ++ax0_ax1_ax2_fused_2_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 8192) + (((int)threadIdx.y) * 4096)) + (((int)threadIdx.z) * 2048)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2_1 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 8192))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 8192) + (((int)threadIdx.y) * 4096)) + (((int)threadIdx.z) * 2048)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ (((ax0_ax1_ax2_fused_2_1 & 1) * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 8192)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int)blockIdx.y) * 2097152) + (((int)threadIdx.y) * 1048576)) + (((int)threadIdx.z) * 524288)) + (ax0_ax1_ax2_fused_2_1 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (ax3_0_0 * 64)) + ((((int)threadIdx.x) & 3) * 16)) + 64))), "n"(16)
    );
  }
      }
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_fused_0_1 = 0; ax0_ax1_ax2_ax3_fused_0_1 < 2; ++ax0_ax1_ax2_ax3_fused_0_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((((ax3_0_0 + 1) & 1) * 4096) + (ax0_ax1_ax2_ax3_fused_0_1 * 2048)) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((((ax3_0_0 + 1) & 1) * 4096) + (ax0_ax1_ax2_ax3_fused_0_1 * 2048)) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + (((((((((((int)blockIdx.z) * 8388608) + (((int)blockIdx.x) * 1048576)) + (ax0_ax1_ax2_ax3_fused_0_1 * 524288)) + (((int)threadIdx.z) * 262144)) + (((int)threadIdx.y) * 131072)) + ((((int)threadIdx.x) >> 4) * 65536)) + (ax3_0_0 * 256)) + ((((int)threadIdx.x) & 15) * 16)) + 256))), "n"(16)
    );
  }
      }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 1;");

      __syncthreads();
      for (int ax1_ax2_ax3_ax4_0_fused_0 = 0; ax1_ax2_ax3_ax4_0_fused_0 < 8; ++ax1_ax2_ax3_ax4_0_fused_0) {
        B_local[0] = *(int*)(((signed char*)buf_shmem) + ((((((ax3_0_0 & 1) * 4096) + (ax1_ax2_ax3_ax4_0_fused_0 * 512)) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)));
        decode_i2s_to_i8s(B_local, B_reindex_reindex_local, 16);
        *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 24576)) = B_reindex_reindex_local[0];
      }
      __syncthreads();
      for (int ax3_0_1 = 0; ax3_0_1 < 2; ++ax3_0_1) {
        for (int ax1_0 = 0; ax1_0 < 4; ++ax1_0) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 8192) + (((int)threadIdx.y) * 4096)) + (ax1_0 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 8192)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 8192) + (((int)threadIdx.y) * 4096)) + (ax1_0 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 8192)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax1 = 0; ax1 < 8; ++ax1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 8192) + (ax1 * 1024)) + (ax3_0_1 * 512)) + 24576)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 8192) + (ax1 * 1024)) + (ax3_0_1 * 512)) + 24576)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax1_0_3 = 0; ax1_0_3 < 4; ++ax1_0_3) {
          for (int ax2_0_3 = 0; ax2_0_3 < 8; ++ax2_0_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 64) + (ax2_0_3 * 8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 64) + (ax2_0_3 * 8)) + 4)))[3]));
  }
          }
        }
      }
    }
__asm__ __volatile__("cp.async.wait_group 0;");

    __syncthreads();
    for (int ax1_ax2_ax3_ax4_0_fused_0_1 = 0; ax1_ax2_ax3_ax4_0_fused_0_1 < 8; ++ax1_ax2_ax3_ax4_0_fused_0_1) {
      B_local_1[0] = *(int*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 512) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)) + 4096));
      decode_i2s_to_i8s(B_local_1, B_reindex_reindex_local_1, 16);
      *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 24576)) = B_reindex_reindex_local_1[0];
    }
    __syncthreads();
    for (int ax3_0_1_1 = 0; ax3_0_1_1 < 2; ++ax3_0_1_1) {
      for (int ax1_0_1 = 0; ax1_0_1 < 4; ++ax1_0_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 4096) + (ax1_0_1 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 16384)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 4096) + (ax1_0_1 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 16384)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax1_1 = 0; ax1_1 < 8; ++ax1_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 8192) + (ax1_1 * 1024)) + (ax3_0_1_1 * 512)) + 24576)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 8192) + (ax1_1 * 1024)) + (ax3_0_1_1 * 512)) + 24576)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax1_0_3_1 = 0; ax1_0_3_1 < 4; ++ax1_0_3_1) {
        for (int ax2_0_3_1 = 0; ax2_0_3_1 < 8; ++ax2_0_3_1) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 64) + (ax2_0_3_1 * 8)) + 4)))[3]));
  }
        }
      }
    }
    for (int ax0 = 0; ax0 < 4; ++ax0) {
      __syncthreads();
      for (int ax1_2 = 0; ax1_2 < 8; ++ax1_2) {
        for (int local_id = 0; local_id < 8; ++local_id) {
(&(((int*)buf_shmem)[((((((int)threadIdx.y) * 16384) + (((int)threadIdx.z) * 2048)) + (ax1_2 * 256)) + 10240)]))[((((((local_id % 4) / 2) * 8) + (threadIdx.x / 4)) * 16) + ((((local_id / 4) * 8) + ((threadIdx.x % 4) * 2)) + (local_id % 2)))] = C_reindex_shared_warp[((ax0 * 64) + (ax1_2 * 8)) + local_id];
}
;
      }
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_ax4_fused_0 = 0; ax0_ax1_ax2_ax3_ax4_fused_0 < 16; ++ax0_ax1_ax2_ax3_ax4_fused_0) {
        *(int4*)(C + ((((((((((((int)blockIdx.y) * 2097152) + (((int)threadIdx.y) * 1048576)) + (ax0 * 262144)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 & 1) * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (((int)blockIdx.z) * 2048)) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.z) * 128)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4))) = *(int4*)(((int*)buf_shmem) + (((((((int)threadIdx.y) * 16384) + (((int)threadIdx.z) * 2048)) + (ax0_ax1_ax2_ax3_ax4_fused_0 * 128)) + (((int)threadIdx.x) * 4)) + 10240));
      }
    }
  }
}


Compilation error:
/tmp/tmpwy1too3e/tvm_kernels.cu(52): warning #177-D: function "__dp4a(int, unsigned int, int)" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/tmp/tmpwy1too3e/tvm_kernels.cu(46): warning #177-D: function "__dp4a(unsigned int, int, int)" was declared but never referenced

ptxas error   : Entry function 'default_function_kernel' uses too much shared data (0x1e000 bytes, 0xc000 max)


[FastDlight] LocalBuilder: An exception occurred  Traceback (most recent call last):
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/t-leiwang/ladder_workspace/BitBLAS/python/bitblas/base/utils.py", line 201, in _build
    rt_mod = tvm.build(mod["main"], target=arch.target)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/driver/build_module.py", line 294, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 204, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 128, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: template <typename T1, typename T2>
__device__ void decode_i2s_to_i8s(T1 *_i2s, T2 *_i8s, const int N = 16)
{
  // convert 8 int2b_t to 8 int8b_t -> 2 int32
  uint *i8s = reinterpret_cast<uint *>(_i8s);

  // i2s = {e7,e6,e5,e4,e3,e2,e1,e0}
  // also require interleave {e7,e3,e6,e2,e5,e1,e4,e0}
  uint const i2s = *_i2s;

  // First, we extract the i4s and construct an intermediate fp16 number.
  static constexpr uint immLut = (0xf0 & 0xcc) | 0xaa;     // 0b11101010
  static constexpr uint BOTTOM_MASK = 0x03030303;          // 0xf -> 0b11 select 0,3
  static constexpr uint I4s_TO_I8s_MAGIC_NUM = 0x00000000; // 1024

#pragma unroll
  for (int i = 0; i < (N / 2); i++)
  {
    asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                 : "=r"(i8s[i])
                 : "r"(i2s >> (2 * i)), "n"(BOTTOM_MASK), "n"(I4s_TO_I8s_MAGIC_NUM), "n"(immLut));
  }
}
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 610)
#include <sm_61_intrinsics.h>


#if defined(__CUDACC_RTC__)
#define __SM_61_INTRINSICS_DECL__ __device__
#else /* !__CUDACC_RTC__ */
#define __SM_61_INTRINSICS_DECL__ static __device__ __inline__
#endif /* __CUDACC_RTC__ */

#ifndef __CUDA_ARCH__
#define __DEF_IF_HOST { }
#else  /* !__CUDA_ARCH__ */
#define __DEF_IF_HOST ;
#endif /* __CUDA_ARCH__ */

__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) __DEF_IF_HOST
__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) __DEF_IF_HOST

#undef __DEF_IF_HOST

#if !defined(__CUDACC_RTC__) && defined(__CUDA_ARCH__)
__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) {
    int ret;
    asm volatile ("dp4a.u32.s32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}

__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) {
    int ret;
    asm volatile ("dp4a.s32.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}
#endif /* !__CUDACC_RTC__ && defined(__CUDA_ARCH__) */

#undef __SM_61_INTRINSICS_DECL__

#endif
__forceinline__ __device__ unsigned int
cast_smem_ptr_to_int(const void* const smem_ptr)
{
  unsigned int smem_int;
  asm volatile ("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; cvt.u32.u64 %0, smem_int; }"
    : "=r"(smem_int) : "l"(smem_ptr));
  return smem_int;
}

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif

#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ == 800) 
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 1
#else
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 0
#endif
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C);
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C) {

        const int MAX_BLOCK_N = 10;
        const auto baseBlockIdx = blockIdx.x + gridDim.x *blockIdx.y;
        const auto totalPanel = (gridDim.x * gridDim.y +MAX_BLOCK_N * gridDim.x - 1) / (MAX_BLOCK_N * gridDim.x);
        const auto totalBlock = gridDim.x * gridDim.y;
        const auto panelIdx = baseBlockIdx / (MAX_BLOCK_N *gridDim.x);
        const auto strideLd = panelIdx + 1 < totalPanel ?MAX_BLOCK_N : (totalBlock - panelIdx * (MAX_BLOCK_N *gridDim.x)) / gridDim.x;
        const auto bx = (panelIdx & 1) ? gridDim.x -(baseBlockIdx - panelIdx * MAX_BLOCK_N * gridDim.x) /strideLd - 1 : (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) / strideLd;
        const auto by = (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) % strideLd + panelIdx * MAX_BLOCK_N;
        const auto bz = blockIdx.z;
        const dim3 blockIdx(bx, by, bz);
      __shared__ uchar buf_shmem[84992];
  int C_reindex_shared_warp[32];
  int B_local[1];
  int4 B_reindex_reindex_local[1];
  signed char A_reindex_shared_warp[64];
  signed char B_reindex_reindex_shared_warp[16];
  int B_local_1[1];
  int4 B_reindex_reindex_local_1[1];
  signed char A_reindex_shared_warp_1[64];
  signed char B_reindex_reindex_shared_warp_1[16];
  for (int var = 0; var < 1; ++var) {
    for (int ax1_0_3_init = 0; ax1_0_3_init < 4; ++ax1_0_3_init) {
      for (int i = 0; i < 8; ++i) {
C_reindex_shared_warp[(ax1_0_3_init * 8) + i] = 0.0;}
;
    }
    #pragma unroll
    for (int ax0_ax1_ax2_fused_2 = 0; ax0_ax1_ax2_fused_2 < 16; ++ax0_ax1_ax2_fused_2) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((((int)threadIdx.y) * 8192) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((ax0_ax1_ax2_fused_2 & 3) * 4) + (((int)threadIdx.x) >> 3))) * 16)) + 19456))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((((int)threadIdx.y) * 8192) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((ax0_ax1_ax2_fused_2 & 3) * 4) + (((int)threadIdx.x) >> 3))) * 16)) + 19456)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + (((((((int)blockIdx.y) * 4194304) + (((int)threadIdx.y) * 1048576)) + (ax0_ax1_ax2_fused_2 * 65536)) + ((((int)threadIdx.x) >> 3) * 16384)) + ((((int)threadIdx.x) & 7) * 16)))), "n"(16)
    );
  }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_fused_0 = 0; ax0_ax1_ax2_ax3_fused_0 < 1; ++ax0_ax1_ax2_ax3_fused_0) {
      if (((int)threadIdx.y) < 1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((int)blockIdx.z) * 524288) + (((int)blockIdx.x) * 65536)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))), "n"(16)
    );
  }
      }
    }
__asm__ __volatile__("cp.async.commit_group;");

    for (int ax3_0_0 = 0; ax3_0_0 < 127; ++ax3_0_0) {
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_fused_2_1 = 0; ax0_ax1_ax2_fused_2_1 < 16; ++ax0_ax1_ax2_fused_2_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((ax3_0_0 + 1) & 1) * 32768) + (((int)threadIdx.y) * 8192)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((ax0_ax1_ax2_fused_2_1 & 3) * 4) + (((int)threadIdx.x) >> 3))) * 16)) + 19456))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((ax3_0_0 + 1) & 1) * 32768) + (((int)threadIdx.y) * 8192)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((ax0_ax1_ax2_fused_2_1 & 3) * 4) + (((int)threadIdx.x) >> 3))) * 16)) + 19456)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + (((((((((int)blockIdx.y) * 4194304) + (((int)threadIdx.y) * 1048576)) + (ax0_ax1_ax2_fused_2_1 * 65536)) + ((((int)threadIdx.x) >> 3) * 16384)) + (ax3_0_0 * 128)) + ((((int)threadIdx.x) & 7) * 16)) + 128))), "n"(16)
    );
  }
      }
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_fused_0_1 = 0; ax0_ax1_ax2_ax3_fused_0_1 < 1; ++ax0_ax1_ax2_ax3_fused_0_1) {
        if (((int)threadIdx.y) < 1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((ax3_0_0 + 1) & 1) * 512) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((ax3_0_0 + 1) & 1) * 512) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((int)blockIdx.z) * 524288) + (((int)blockIdx.x) * 65536)) + (ax3_0_0 * 512)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)) + 512))), "n"(16)
    );
  }
        }
      }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 1;");

      __syncthreads();
      B_local[0] = *(int*)(((signed char*)buf_shmem) + ((((ax3_0_0 & 1) * 512) + (((int)threadIdx.y) * 128)) + (((int)threadIdx.x) * 4)));
      decode_i2s_to_i8s(B_local, B_reindex_reindex_local, 16);
      *(int4*)(((signed char*)buf_shmem) + (((((int)threadIdx.y) * 512) + (((int)threadIdx.x) * 16)) + 1024)) = B_reindex_reindex_local[0];
      __syncthreads();
      for (int ax3_0_1 = 0; ax3_0_1 < 4; ++ax3_0_1) {
        for (int ax1_0 = 0; ax1_0 < 4; ++ax1_0) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 32768) + (((int)threadIdx.y) * 8192)) + (ax1_0 * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 19456)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 32768) + (((int)threadIdx.y) * 8192)) + (ax1_0 * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 19456)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[3])
      : "r"(addr)
    );
  }
        }

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((ax3_0_1 * 512) + 1024)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((ax3_0_1 * 512) + 1024)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[3])
      : "r"(addr)
    );
  }
        for (int ax1_0_3 = 0; ax1_0_3 < 4; ++ax1_0_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[0]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[1]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[2]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 0))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[0]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[2]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3 * 8)))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 8))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + 8))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 8) + 4)))[3]));
  }
        }
      }
    }
__asm__ __volatile__("cp.async.wait_group 0;");

    __syncthreads();
    B_local_1[0] = *(int*)(((signed char*)buf_shmem) + (((((int)threadIdx.y) * 128) + (((int)threadIdx.x) * 4)) + 512));
    decode_i2s_to_i8s(B_local_1, B_reindex_reindex_local_1, 16);
    *(int4*)(((signed char*)buf_shmem) + (((((int)threadIdx.y) * 512) + (((int)threadIdx.x) * 16)) + 1024)) = B_reindex_reindex_local_1[0];
    __syncthreads();
    for (int ax3_0_1_1 = 0; ax3_0_1_1 < 4; ++ax3_0_1_1) {
      for (int ax1_0_1 = 0; ax1_0_1 < 4; ++ax1_0_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 8192) + (ax1_0_1 * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 52224)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 8192) + (ax1_0_1 * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 52224)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((ax3_0_1_1 * 512) + 1024)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((ax3_0_1_1 * 512) + 1024)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[3])
      : "r"(addr)
    );
  }
      for (int ax1_0_3_1 = 0; ax1_0_3_1 < 4; ++ax1_0_3_1) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[0]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[1]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[2]), "=r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 0))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[0]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[2]), "r"(((int *)(C_reindex_shared_warp + (ax1_0_3_1 * 8)))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 8))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + 8))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 8) + 4)))[3]));
  }
      }
    }
    for (int ax0 = 0; ax0 < 4; ++ax0) {
      for (int local_id = 0; local_id < 8; ++local_id) {
(&(((int*)buf_shmem)[(((((int)threadIdx.y) * 1024) + (ax0 * 256)) + 768)]))[((((((local_id % 4) / 2) * 8) + (threadIdx.x / 4)) * 16) + ((((local_id / 4) * 8) + ((threadIdx.x % 4) * 2)) + (local_id % 2)))] = C_reindex_shared_warp[(ax0 * 8) + local_id];
}
;
    }
    __syncthreads();
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_ax4_fused_0 = 0; ax0_ax1_ax2_ax3_ax4_fused_0 < 8; ++ax0_ax1_ax2_ax3_ax4_fused_0) {
      *(int4*)(C + (((((((((int)blockIdx.y) * 4194304) + (((int)threadIdx.y) * 1048576)) + (ax0_ax1_ax2_ax3_ax4_fused_0 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (((int)blockIdx.z) * 128)) + (((int)blockIdx.x) * 16)) + ((((int)threadIdx.x) & 3) * 4))) = *(int4*)(((int*)buf_shmem) + ((((((int)threadIdx.y) * 1024) + (ax0_ax1_ax2_ax3_ax4_fused_0 * 128)) + (((int)threadIdx.x) * 4)) + 768));
    }
  }
}


Compilation error:
/tmp/tmpsits3gou/tvm_kernels.cu(52): warning #177-D: function "__dp4a(int, unsigned int, int)" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/tmp/tmpsits3gou/tvm_kernels.cu(46): warning #177-D: function "__dp4a(unsigned int, int, int)" was declared but never referenced

ptxas error   : Entry function 'default_function_kernel' uses too much shared data (0x14c00 bytes, 0xc000 max)


[FastDlight] LocalBuilder: An exception occurred  Traceback (most recent call last):
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/t-leiwang/ladder_workspace/BitBLAS/python/bitblas/base/utils.py", line 201, in _build
    rt_mod = tvm.build(mod["main"], target=arch.target)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/driver/build_module.py", line 294, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/_ctypes/packed_func.py", line 239, in __call__
    raise_last_ffi_error()
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/_ffi/base.py", line 481, in raise_last_ffi_error
    raise py_err
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 204, in tvm_callback_cuda_compile
    ptx = compile_cuda(code, target_format="fatbin")
  File "/home/t-leiwang/mlc_workspace/unity/python/tvm/contrib/nvcc.py", line 128, in compile_cuda
    raise RuntimeError(msg)
RuntimeError: template <typename T1, typename T2>
__device__ void decode_i2s_to_i8s(T1 *_i2s, T2 *_i8s, const int N = 16)
{
  // convert 8 int2b_t to 8 int8b_t -> 2 int32
  uint *i8s = reinterpret_cast<uint *>(_i8s);

  // i2s = {e7,e6,e5,e4,e3,e2,e1,e0}
  // also require interleave {e7,e3,e6,e2,e5,e1,e4,e0}
  uint const i2s = *_i2s;

  // First, we extract the i4s and construct an intermediate fp16 number.
  static constexpr uint immLut = (0xf0 & 0xcc) | 0xaa;     // 0b11101010
  static constexpr uint BOTTOM_MASK = 0x03030303;          // 0xf -> 0b11 select 0,3
  static constexpr uint I4s_TO_I8s_MAGIC_NUM = 0x00000000; // 1024

#pragma unroll
  for (int i = 0; i < (N / 2); i++)
  {
    asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                 : "=r"(i8s[i])
                 : "r"(i2s >> (2 * i)), "n"(BOTTOM_MASK), "n"(I4s_TO_I8s_MAGIC_NUM), "n"(immLut));
  }
}
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 610)
#include <sm_61_intrinsics.h>


#if defined(__CUDACC_RTC__)
#define __SM_61_INTRINSICS_DECL__ __device__
#else /* !__CUDACC_RTC__ */
#define __SM_61_INTRINSICS_DECL__ static __device__ __inline__
#endif /* __CUDACC_RTC__ */

#ifndef __CUDA_ARCH__
#define __DEF_IF_HOST { }
#else  /* !__CUDA_ARCH__ */
#define __DEF_IF_HOST ;
#endif /* __CUDA_ARCH__ */

__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) __DEF_IF_HOST
__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) __DEF_IF_HOST

#undef __DEF_IF_HOST

#if !defined(__CUDACC_RTC__) && defined(__CUDA_ARCH__)
__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) {
    int ret;
    asm volatile ("dp4a.u32.s32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}

__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) {
    int ret;
    asm volatile ("dp4a.s32.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}
#endif /* !__CUDACC_RTC__ && defined(__CUDA_ARCH__) */

#undef __SM_61_INTRINSICS_DECL__

#endif
__forceinline__ __device__ unsigned int
cast_smem_ptr_to_int(const void* const smem_ptr)
{
  unsigned int smem_int;
  asm volatile ("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; cvt.u32.u64 %0, smem_int; }"
    : "=r"(smem_int) : "l"(smem_ptr));
  return smem_int;
}

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif

#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ == 800) 
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 1
#else
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 0
#endif
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C);
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C) {

        const int MAX_BLOCK_N = 10;
        const auto baseBlockIdx = blockIdx.x + gridDim.x *blockIdx.y;
        const auto totalPanel = (gridDim.x * gridDim.y +MAX_BLOCK_N * gridDim.x - 1) / (MAX_BLOCK_N * gridDim.x);
        const auto totalBlock = gridDim.x * gridDim.y;
        const auto panelIdx = baseBlockIdx / (MAX_BLOCK_N *gridDim.x);
        const auto strideLd = panelIdx + 1 < totalPanel ?MAX_BLOCK_N : (totalBlock - panelIdx * (MAX_BLOCK_N *gridDim.x)) / gridDim.x;
        const auto bx = (panelIdx & 1) ? gridDim.x -(baseBlockIdx - panelIdx * MAX_BLOCK_N * gridDim.x) /strideLd - 1 : (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) / strideLd;
        const auto by = (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) % strideLd + panelIdx * MAX_BLOCK_N;
        const auto bz = blockIdx.z;
        const dim3 blockIdx(bx, by, bz);
      __shared__ uchar buf_shmem[90112];
  int C_reindex_shared_warp[64];
  int B_local[1];
  int4 B_reindex_reindex_local[1];
  signed char A_reindex_shared_warp[16];
  signed char B_reindex_reindex_shared_warp[128];
  int B_local_1[1];
  int4 B_reindex_reindex_local_1[1];
  signed char A_reindex_shared_warp_1[16];
  signed char B_reindex_reindex_shared_warp_1[128];
  for (int var = 0; var < 1; ++var) {
    for (int ax2_0_3_init = 0; ax2_0_3_init < 8; ++ax2_0_3_init) {
      for (int i = 0; i < 8; ++i) {
C_reindex_shared_warp[(ax2_0_3_init * 8) + i] = 0.0;}
;
    }
    #pragma unroll
    for (int ax0_ax1_ax2_fused_2 = 0; ax0_ax1_ax2_fused_2 < 2; ++ax0_ax1_ax2_fused_2) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((((int)threadIdx.y) * 2048) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((((int)threadIdx.z) * 8) + (ax0_ax1_ax2_fused_2 * 4)) + (((int)threadIdx.x) >> 3))) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((((int)threadIdx.y) * 2048) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((((int)threadIdx.z) * 8) + (ax0_ax1_ax2_fused_2 * 4)) + (((int)threadIdx.x) >> 3))) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((int)blockIdx.y) * 524288) + (((int)threadIdx.y) * 262144)) + (((int)threadIdx.z) * 131072)) + (ax0_ax1_ax2_fused_2 * 65536)) + ((((int)threadIdx.x) >> 3) * 16384)) + ((((int)threadIdx.x) & 7) * 16)))), "n"(16)
    );
  }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_fused_0 = 0; ax0_ax1_ax2_ax3_fused_0 < 4; ++ax0_ax1_ax2_ax3_fused_0) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((ax0_ax1_ax2_ax3_fused_0 * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)) + 8192))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((ax0_ax1_ax2_ax3_fused_0 * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)) + 8192)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((int)blockIdx.z) * 8388608) + (((int)blockIdx.x) * 1048576)) + (ax0_ax1_ax2_ax3_fused_0 * 262144)) + (((int)threadIdx.z) * 131072)) + (((int)threadIdx.y) * 65536)) + (((int)threadIdx.x) * 16)))), "n"(16)
    );
  }
    }
__asm__ __volatile__("cp.async.commit_group;");

    for (int ax3_0_0 = 0; ax3_0_0 < 127; ++ax3_0_0) {
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_fused_2_1 = 0; ax0_ax1_ax2_fused_2_1 < 2; ++ax0_ax1_ax2_fused_2_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((ax3_0_0 + 1) & 1) * 4096) + (((int)threadIdx.y) * 2048)) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((((int)threadIdx.z) * 8) + (ax0_ax1_ax2_fused_2_1 * 4)) + (((int)threadIdx.x) >> 3))) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((ax3_0_0 + 1) & 1) * 4096) + (((int)threadIdx.y) * 2048)) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 3) * 128)) + (((((int)threadIdx.x) & 7) ^ (((((int)threadIdx.z) * 8) + (ax0_ax1_ax2_fused_2_1 * 4)) + (((int)threadIdx.x) >> 3))) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int)blockIdx.y) * 524288) + (((int)threadIdx.y) * 262144)) + (((int)threadIdx.z) * 131072)) + (ax0_ax1_ax2_fused_2_1 * 65536)) + ((((int)threadIdx.x) >> 3) * 16384)) + (ax3_0_0 * 128)) + ((((int)threadIdx.x) & 7) * 16)) + 128))), "n"(16)
    );
  }
      }
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_fused_0_1 = 0; ax0_ax1_ax2_ax3_fused_0_1 < 4; ++ax0_ax1_ax2_ax3_fused_0_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((ax3_0_0 + 1) & 1) * 8192) + (ax0_ax1_ax2_ax3_fused_0_1 * 2048)) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)) + 8192))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((ax3_0_0 + 1) & 1) * 8192) + (ax0_ax1_ax2_ax3_fused_0_1 * 2048)) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)) + 8192)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int)blockIdx.z) * 8388608) + (((int)blockIdx.x) * 1048576)) + (ax0_ax1_ax2_ax3_fused_0_1 * 262144)) + (((int)threadIdx.z) * 131072)) + (((int)threadIdx.y) * 65536)) + (ax3_0_0 * 512)) + (((int)threadIdx.x) * 16)) + 512))), "n"(16)
    );
  }
      }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 1;");

      __syncthreads();
      for (int ax1_ax2_ax3_ax4_0_fused_0 = 0; ax1_ax2_ax3_ax4_0_fused_0 < 16; ++ax1_ax2_ax3_ax4_0_fused_0) {
        B_local[0] = *(int*)(((signed char*)buf_shmem) + (((((((ax3_0_0 & 1) * 8192) + (ax1_ax2_ax3_ax4_0_fused_0 * 512)) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)) + 8192));
        decode_i2s_to_i8s(B_local, B_reindex_reindex_local, 16);
        *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 24576)) = B_reindex_reindex_local[0];
      }
      __syncthreads();
      for (int ax3_0_1 = 0; ax3_0_1 < 4; ++ax3_0_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((ax3_0_0 & 1) * 4096) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16))])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((ax3_0_0 & 1) * 4096) + (((int)threadIdx.y) * 2048)) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16))])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp + 0))[0]), "=r"(((unsigned *)(A_reindex_shared_warp + 0))[1]), "=r"(((unsigned *)(A_reindex_shared_warp + 0))[2]), "=r"(((unsigned *)(A_reindex_shared_warp + 0))[3])
      : "r"(addr)
    );
  }
        for (int ax1 = 0; ax1 < 8; ++ax1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 16384) + (ax1 * 2048)) + (ax3_0_1 * 512)) + 24576)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 16384) + (ax1 * 2048)) + (ax3_0_1 * 512)) + 24576)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax2_0_3 = 0; ax2_0_3 < 8; ++ax2_0_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (ax2_0_3 * 8)))[0]), "=r"(((int *)(C_reindex_shared_warp + (ax2_0_3 * 8)))[1]), "=r"(((int *)(C_reindex_shared_warp + (ax2_0_3 * 8)))[2]), "=r"(((int *)(C_reindex_shared_warp + (ax2_0_3 * 8)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + 0))[0]), "r"(((unsigned *)(A_reindex_shared_warp + 0))[1]), "r"(((unsigned *)(A_reindex_shared_warp + 0))[2]), "r"(((unsigned *)(A_reindex_shared_warp + 0))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + (ax2_0_3 * 8)))[0]), "r"(((int *)(C_reindex_shared_warp + (ax2_0_3 * 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (ax2_0_3 * 8)))[2]), "r"(((int *)(C_reindex_shared_warp + (ax2_0_3 * 8)))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax2_0_3 * 8) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax2_0_3 * 8) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax2_0_3 * 8) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax2_0_3 * 8) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + 0))[0]), "r"(((unsigned *)(A_reindex_shared_warp + 0))[1]), "r"(((unsigned *)(A_reindex_shared_warp + 0))[2]), "r"(((unsigned *)(A_reindex_shared_warp + 0))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax2_0_3 * 8) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax2_0_3 * 8) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax2_0_3 * 8) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax2_0_3 * 8) + 4)))[3]));
  }
        }
      }
    }
__asm__ __volatile__("cp.async.wait_group 0;");

    __syncthreads();
    for (int ax1_ax2_ax3_ax4_0_fused_0_1 = 0; ax1_ax2_ax3_ax4_0_fused_0_1 < 16; ++ax1_ax2_ax3_ax4_0_fused_0_1) {
      B_local_1[0] = *(int*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 512) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)) + 16384));
      decode_i2s_to_i8s(B_local_1, B_reindex_reindex_local_1, 16);
      *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 24576)) = B_reindex_reindex_local_1[0];
    }
    __syncthreads();
    for (int ax3_0_1_1 = 0; ax3_0_1_1 < 4; ++ax3_0_1_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.y) * 2048) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 4096)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.y) * 2048) + ((((int)threadIdx.x) & 15) * 128)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ (((int)threadIdx.x) & 15)) * 16)) + 4096)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp_1 + 0))[0]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + 0))[1]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + 0))[2]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + 0))[3])
      : "r"(addr)
    );
  }
      for (int ax1_1 = 0; ax1_1 < 8; ++ax1_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 16384) + (ax1_1 * 2048)) + (ax3_0_1_1 * 512)) + 24576)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 16384) + (ax1_1 * 2048)) + (ax3_0_1_1 * 512)) + 24576)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax2_0_3_1 = 0; ax2_0_3_1 < 8; ++ax2_0_3_1) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (ax2_0_3_1 * 8)))[0]), "=r"(((int *)(C_reindex_shared_warp + (ax2_0_3_1 * 8)))[1]), "=r"(((int *)(C_reindex_shared_warp + (ax2_0_3_1 * 8)))[2]), "=r"(((int *)(C_reindex_shared_warp + (ax2_0_3_1 * 8)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + 0))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + 0))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + 0))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + 0))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + (ax2_0_3_1 * 8)))[0]), "r"(((int *)(C_reindex_shared_warp + (ax2_0_3_1 * 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (ax2_0_3_1 * 8)))[2]), "r"(((int *)(C_reindex_shared_warp + (ax2_0_3_1 * 8)))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax2_0_3_1 * 8) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax2_0_3_1 * 8) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax2_0_3_1 * 8) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax2_0_3_1 * 8) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + 0))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + 0))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + 0))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + 0))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax2_0_3_1 * 8) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax2_0_3_1 * 8) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax2_0_3_1 * 8) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax2_0_3_1 * 8) + 4)))[3]));
  }
      }
    }
    for (int ax0 = 0; ax0 < 8; ++ax0) {
      for (int local_id = 0; local_id < 8; ++local_id) {
(&(((int*)buf_shmem)[((((((int)threadIdx.y) * 4096) + (((int)threadIdx.z) * 2048)) + (ax0 * 256)) + 14336)]))[((((((local_id % 4) / 2) * 8) + (threadIdx.x / 4)) * 16) + ((((local_id / 4) * 8) + ((threadIdx.x % 4) * 2)) + (local_id % 2)))] = C_reindex_shared_warp[(ax0 * 8) + local_id];
}
;
    }
    __syncthreads();
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_ax4_fused_0 = 0; ax0_ax1_ax2_ax3_ax4_fused_0 < 16; ++ax0_ax1_ax2_ax3_ax4_fused_0) {
      *(int4*)(C + (((((((((((int)blockIdx.y) * 524288) + (((int)threadIdx.y) * 262144)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 & 1) * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (((int)blockIdx.z) * 2048)) + (((int)blockIdx.x) * 256)) + (((int)threadIdx.z) * 128)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4))) = *(int4*)(((int*)buf_shmem) + (((((((int)threadIdx.y) * 4096) + (((int)threadIdx.z) * 2048)) + (ax0_ax1_ax2_ax3_ax4_fused_0 * 128)) + (((int)threadIdx.x) * 4)) + 14336));
    }
  }
}


Compilation error:
/tmp/tmpg804_j9j/tvm_kernels.cu(52): warning #177-D: function "__dp4a(int, unsigned int, int)" was declared but never referenced

Remark: The warnings can be suppressed with "-diag-suppress <warning-number>"

/tmp/tmpg804_j9j/tvm_kernels.cu(46): warning #177-D: function "__dp4a(unsigned int, int, int)" was declared but never referenced

ptxas error   : Entry function 'default_function_kernel' uses too much shared data (0x16000 bytes, 0xc000 max)


[FastDlight] Evaluation with config  {'block': [128, 64], 'warp': [64, 32], 'rstep': [64], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Time cost of this config: 27.207 ms
[FastDlight] Evaluation with config  {'block': [256, 64], 'warp': [128, 32], 'rstep': [64], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Time cost of this config: 29.398 ms
[FastDlight] Evaluation with config  {'block': [64, 64], 'warp': [32, 32], 'rstep': [64], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Time cost of this config: 35.068 ms
[FastDlight] Evaluation with config  {'block': [64, 32], 'warp': [32, 16], 'rstep': [256], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Time cost of this config: 61.230 ms
[FastDlight] Evaluation with config  {'block': [64, 16], 'warp': [16, 16], 'rstep': [256], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Time cost of this config: 64.637 ms
[FastDlight] Evaluation with config  {'block': [32, 128], 'warp': [16, 64], 'rstep': [128], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Time cost of this config: 35.104 ms
[FastDlight] Evaluation with config  {'block': [32, 64], 'warp': [16, 32], 'rstep': [256], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Time cost of this config: 39.622 ms
[FastDlight] Evaluation with config  {'block': [32, 32], 'warp': [16, 16], 'rstep': [256], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Time cost of this config: 52.153 ms
[FastDlight] Evaluation with config  {'block': [16, 128], 'warp': [16, 32], 'rstep': [128], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Time cost of this config: 49.908 ms
[FastDlight] Evaluation with config  {'block': [16, 64], 'warp': [16, 16], 'rstep': [256], 'use_tc': True, 'vectorize': {'A_reindex': 16, 'B_reindex_reindex': 16}}
[FastDlight] Time cost of this config: 58.419 ms
[FastDlight] The best latency of top 1 is 27.207 ms
[FastDlight] The best latency of top 1 is 27.207 ms
template <typename T1, typename T2>
__device__ void decode_i2s_to_i8s(T1 *_i2s, T2 *_i8s, const int N = 16)
{
  // convert 8 int2b_t to 8 int8b_t -> 2 int32
  uint *i8s = reinterpret_cast<uint *>(_i8s);

  // i2s = {e7,e6,e5,e4,e3,e2,e1,e0}
  // also require interleave {e7,e3,e6,e2,e5,e1,e4,e0}
  uint const i2s = *_i2s;

  // First, we extract the i4s and construct an intermediate fp16 number.
  static constexpr uint immLut = (0xf0 & 0xcc) | 0xaa;     // 0b11101010
  static constexpr uint BOTTOM_MASK = 0x03030303;          // 0xf -> 0b11 select 0,3
  static constexpr uint I4s_TO_I8s_MAGIC_NUM = 0x00000000; // 1024

#pragma unroll
  for (int i = 0; i < (N / 2); i++)
  {
    asm volatile("lop3.b32 %0, %1, %2, %3, %4;\n"
                 : "=r"(i8s[i])
                 : "r"(i2s >> (2 * i)), "n"(BOTTOM_MASK), "n"(I4s_TO_I8s_MAGIC_NUM), "n"(immLut));
  }
}
#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 610)
#include <sm_61_intrinsics.h>


#if defined(__CUDACC_RTC__)
#define __SM_61_INTRINSICS_DECL__ __device__
#else /* !__CUDACC_RTC__ */
#define __SM_61_INTRINSICS_DECL__ static __device__ __inline__
#endif /* __CUDACC_RTC__ */

#ifndef __CUDA_ARCH__
#define __DEF_IF_HOST { }
#else  /* !__CUDA_ARCH__ */
#define __DEF_IF_HOST ;
#endif /* __CUDA_ARCH__ */

__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) __DEF_IF_HOST
__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) __DEF_IF_HOST

#undef __DEF_IF_HOST

#if !defined(__CUDACC_RTC__) && defined(__CUDA_ARCH__)
__SM_61_INTRINSICS_DECL__ int __dp4a(unsigned int srcA, int srcB, int c) {
    int ret;
    asm volatile ("dp4a.u32.s32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}

__SM_61_INTRINSICS_DECL__ int __dp4a(int srcA, unsigned int srcB, int c) {
    int ret;
    asm volatile ("dp4a.s32.u32 %0, %1, %2, %3;" : "=r"(ret) : "r"(srcA), "r"(srcB), "r"(c));
    return ret;
}
#endif /* !__CUDACC_RTC__ && defined(__CUDA_ARCH__) */

#undef __SM_61_INTRINSICS_DECL__

#endif
__forceinline__ __device__ unsigned int
cast_smem_ptr_to_int(const void* const smem_ptr)
{
  unsigned int smem_int;
  asm volatile ("{ .reg .u64 smem_int; cvta.to.shared.u64 smem_int, %1; cvt.u32.u64 %0, smem_int; }"
    : "=r"(smem_int) : "l"(smem_ptr));
  return smem_int;
}

#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \
     (__CUDACC_VER_MAJOR__ > 11))
#define TVM_ENABLE_L2_PREFETCH 1
#else
#define TVM_ENABLE_L2_PREFETCH 0
#endif

#ifdef _WIN32
  using uint = unsigned int;
  using uchar = unsigned char;
  using ushort = unsigned short;
  using int64_t = long long;
  using uint64_t = unsigned long long;
#else
  #define uint unsigned int
  #define uchar unsigned char
  #define ushort unsigned short
  #define int64_t long long
  #define uint64_t unsigned long long
#endif

#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ == 800) 
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 1
#else
#define TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST 0
#endif
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C);
extern "C" __global__ void __launch_bounds__(128) default_function_kernel(signed char* __restrict__ A, signed char* __restrict__ B, int* __restrict__ C) {

        const int MAX_BLOCK_N = 10;
        const auto baseBlockIdx = blockIdx.x + gridDim.x *blockIdx.y;
        const auto totalPanel = (gridDim.x * gridDim.y +MAX_BLOCK_N * gridDim.x - 1) / (MAX_BLOCK_N * gridDim.x);
        const auto totalBlock = gridDim.x * gridDim.y;
        const auto panelIdx = baseBlockIdx / (MAX_BLOCK_N *gridDim.x);
        const auto strideLd = panelIdx + 1 < totalPanel ?MAX_BLOCK_N : (totalBlock - panelIdx * (MAX_BLOCK_N *gridDim.x)) / gridDim.x;
        const auto bx = (panelIdx & 1) ? gridDim.x -(baseBlockIdx - panelIdx * MAX_BLOCK_N * gridDim.x) /strideLd - 1 : (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) / strideLd;
        const auto by = (baseBlockIdx - panelIdx * MAX_BLOCK_N *gridDim.x) % strideLd + panelIdx * MAX_BLOCK_N;
        const auto bz = blockIdx.z;
        const dim3 blockIdx(bx, by, bz);
      __shared__ uchar buf_shmem[45056];
  int C_reindex_shared_warp[64];
  int B_local[1];
  int4 B_reindex_reindex_local[1];
  signed char A_reindex_shared_warp[32];
  signed char B_reindex_reindex_shared_warp[64];
  int B_local_1[1];
  int4 B_reindex_reindex_local_1[1];
  signed char A_reindex_shared_warp_1[32];
  signed char B_reindex_reindex_shared_warp_1[64];
  for (int var = 0; var < 1; ++var) {
    for (int ax1_0_3_init = 0; ax1_0_3_init < 2; ++ax1_0_3_init) {
      for (int ax2_0_3_init = 0; ax2_0_3_init < 4; ++ax2_0_3_init) {
        for (int i = 0; i < 8; ++i) {
C_reindex_shared_warp[((ax1_0_3_init * 32) + (ax2_0_3_init * 8)) + i] = 0.0;}
;
      }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_fused_2 = 0; ax0_ax1_ax2_fused_2 < 2; ++ax0_ax1_ax2_fused_2) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((((int)threadIdx.y) * 2048) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ ((ax0_ax1_ax2_fused_2 * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 4096))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((((int)threadIdx.y) * 2048) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_fused_2 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ ((ax0_ax1_ax2_fused_2 * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 4096)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((int)blockIdx.y) * 1048576) + (((int)threadIdx.y) * 524288)) + (((int)threadIdx.z) * 262144)) + (ax0_ax1_ax2_fused_2 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + ((((int)threadIdx.x) & 3) * 16)))), "n"(16)
    );
  }
    }
    #pragma unroll
    for (int ax0_ax1_ax2_ax3_fused_0 = 0; ax0_ax1_ax2_ax3_fused_0 < 1; ++ax0_ax1_ax2_ax3_fused_0) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((int)threadIdx.z) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((int)threadIdx.z) * 1024) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((int)blockIdx.z) * 4194304) + (((int)blockIdx.x) * 524288)) + (((int)threadIdx.z) * 262144)) + (((int)threadIdx.y) * 131072)) + ((((int)threadIdx.x) >> 4) * 65536)) + ((((int)threadIdx.x) & 15) * 16)))), "n"(16)
    );
  }
    }
__asm__ __volatile__("cp.async.commit_group;");

    for (int ax3_0_0 = 0; ax3_0_0 < 255; ++ax3_0_0) {
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_fused_2_1 = 0; ax0_ax1_ax2_fused_2_1 < 2; ++ax0_ax1_ax2_fused_2_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 4096) + (((int)threadIdx.y) * 2048)) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ ((ax0_ax1_ax2_fused_2_1 * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 4096))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + (((((((((ax3_0_0 + 1) & 1) * 4096) + (((int)threadIdx.y) * 2048)) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_fused_2_1 * 512)) + ((((int)threadIdx.x) >> 2) * 64)) + (((((int)threadIdx.x) & 3) ^ ((ax0_ax1_ax2_fused_2_1 * 2) + (((int)threadIdx.x) >> 4))) * 16)) + 4096)))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(A + ((((((((((int)blockIdx.y) * 1048576) + (((int)threadIdx.y) * 524288)) + (((int)threadIdx.z) * 262144)) + (ax0_ax1_ax2_fused_2_1 * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (ax3_0_0 * 64)) + ((((int)threadIdx.x) & 3) * 16)) + 64))), "n"(16)
    );
  }
      }
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_fused_0_1 = 0; ax0_ax1_ax2_ax3_fused_0_1 < 1; ++ax0_ax1_ax2_ax3_fused_0_1) {

  {
        unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)(buf_shmem + ((((((ax3_0_0 + 1) & 1) * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16)))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)(buf_shmem + ((((((ax3_0_0 + 1) & 1) * 2048) + (((int)threadIdx.z) * 1024)) + (((int)threadIdx.y) * 512)) + (((int)threadIdx.x) * 16))))
    );
#endif
    __asm__ __volatile__(
      #if TVM_ENABLE_L2_PREFETCH
        "cp.async.cg.shared.global.L2::128B [%0], [%1], %2;"
      #else
        "cp.async.cg.shared.global [%0], [%1], %2;"
      #endif
        :: "r"(addr), "l"((void*)(B + ((((((((((int)blockIdx.z) * 4194304) + (((int)blockIdx.x) * 524288)) + (((int)threadIdx.z) * 262144)) + (((int)threadIdx.y) * 131072)) + ((((int)threadIdx.x) >> 4) * 65536)) + (ax3_0_0 * 256)) + ((((int)threadIdx.x) & 15) * 16)) + 256))), "n"(16)
    );
  }
      }
__asm__ __volatile__("cp.async.commit_group;");

__asm__ __volatile__("cp.async.wait_group 1;");

      __syncthreads();
      for (int ax1_ax2_ax3_ax4_0_fused_0 = 0; ax1_ax2_ax3_ax4_0_fused_0 < 4; ++ax1_ax2_ax3_ax4_0_fused_0) {
        B_local[0] = *(int*)(((signed char*)buf_shmem) + ((((((ax3_0_0 & 1) * 2048) + (ax1_ax2_ax3_ax4_0_fused_0 * 512)) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)));
        decode_i2s_to_i8s(B_local, B_reindex_reindex_local, 16);
        *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 12288)) = B_reindex_reindex_local[0];
      }
      __syncthreads();
      for (int ax3_0_1 = 0; ax3_0_1 < 2; ++ax3_0_1) {
        for (int ax1_0 = 0; ax1_0 < 2; ++ax1_0) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 4096) + (((int)threadIdx.y) * 2048)) + (ax1_0 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 4096)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((ax3_0_0 & 1) * 4096) + (((int)threadIdx.y) * 2048)) + (ax1_0 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 4096)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp + (ax1_0 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax1 = 0; ax1 < 4; ++ax1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 4096) + (ax1 * 1024)) + (ax3_0_1 * 512)) + 12288)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 4096) + (ax1 * 1024)) + (ax3_0_1 * 512)) + 12288)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax1 * 16)))[3])
      : "r"(addr)
    );
  }
        }
        for (int ax1_0_3 = 0; ax1_0_3 < 2; ++ax1_0_3) {
          for (int ax2_0_3 = 0; ax2_0_3 < 4; ++ax2_0_3) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + (ax2_0_3 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3 * 32) + (ax2_0_3 * 8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp + (ax1_0_3 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp + ((ax2_0_3 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3 * 32) + (ax2_0_3 * 8)) + 4)))[3]));
  }
          }
        }
      }
    }
__asm__ __volatile__("cp.async.wait_group 0;");

    __syncthreads();
    for (int ax1_ax2_ax3_ax4_0_fused_0_1 = 0; ax1_ax2_ax3_ax4_0_fused_0_1 < 4; ++ax1_ax2_ax3_ax4_0_fused_0_1) {
      B_local_1[0] = *(int*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 512) + (((int)threadIdx.y) * 256)) + (((int)threadIdx.z) * 128)) + (((int)threadIdx.x) * 4)) + 2048));
      decode_i2s_to_i8s(B_local_1, B_reindex_reindex_local_1, 16);
      *(int4*)(((signed char*)buf_shmem) + (((((ax1_ax2_ax3_ax4_0_fused_0_1 * 2048) + (((int)threadIdx.y) * 1024)) + (((int)threadIdx.z) * 512)) + (((int)threadIdx.x) * 16)) + 12288)) = B_reindex_reindex_local_1[0];
    }
    __syncthreads();
    for (int ax3_0_1_1 = 0; ax3_0_1_1 < 2; ++ax3_0_1_1) {
      for (int ax1_0_1 = 0; ax1_0_1 < 2; ++ax1_0_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 2048) + (ax1_0_1 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 8192)])) + 0)));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[(((((((int)threadIdx.y) * 2048) + (ax1_0_1 * 1024)) + ((((int)threadIdx.x) & 15) * 64)) + ((((ax3_0_1_1 * 2) + (((int)threadIdx.x) >> 4)) ^ ((((int)threadIdx.x) & 15) >> 2)) * 16)) + 8192)])) + 0))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[0]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[1]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[2]), "=r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax1_1 = 0; ax1_1 < 4; ++ax1_1) {

  {
    unsigned int addr;
#if TVM_ENBALE_EFFICIENT_SMEM_PTR_CAST
    addr = static_cast<unsigned int>(__cvta_generic_to_shared((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 4096) + (ax1_1 * 1024)) + (ax3_0_1_1 * 512)) + 12288)])) + (((int)threadIdx.x) * 16))));
#else
    __asm__ __volatile__(
      "{ .reg .u64 addr; cvta.to.shared.u64 addr, %1; cvt.u32.u64 %0, addr; }\n"
      : "=r"(addr)
      : "l"((void *)((&(((signed char*)buf_shmem)[((((((int)threadIdx.z) * 4096) + (ax1_1 * 1024)) + (ax3_0_1_1 * 512)) + 12288)])) + (((int)threadIdx.x) * 16)))
    );
#endif
    __asm__ __volatile__(
      "ldmatrix.sync.aligned.m8n8.x4.shared.b16"
      "{%0, %1, %2, %3}, [%4];\n"
      : "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[0]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[1]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[2]), "=r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax1_1 * 16)))[3])
      : "r"(addr)
    );
  }
      }
      for (int ax1_0_3_1 = 0; ax1_0_3_1 < 2; ++ax1_0_3_1) {
        for (int ax2_0_3_1 = 0; ax2_0_3_1 < 4; ++ax2_0_3_1) {

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[0]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[1]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[2]), "=r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + (ax2_0_3_1 * 16)))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[0]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[1]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[2]), "r"(((int *)(C_reindex_shared_warp + ((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8))))[3]));
  }

  {
    __asm__ __volatile__(
      "mma.sync.aligned.m16n8k32.row.col.s32.s8.s8.s32"
      "{%0, %1, %2, %3}, {%4, %5, %6, %7}, {%8, %9}, {%10, %11, %12, %13};\n"
      :  "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[0]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[1]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[2]), "=r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[3])
      : "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[0]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[1]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[2]), "r"(((unsigned *)(A_reindex_shared_warp_1 + (ax1_0_3_1 * 16)))[3]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[0]), "r"(((unsigned *)(B_reindex_reindex_shared_warp_1 + ((ax2_0_3_1 * 16) + 8)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[0]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[1]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[2]), "r"(((int *)(C_reindex_shared_warp + (((ax1_0_3_1 * 32) + (ax2_0_3_1 * 8)) + 4)))[3]));
  }
        }
      }
    }
    for (int ax0 = 0; ax0 < 2; ++ax0) {
      __syncthreads();
      for (int ax1_2 = 0; ax1_2 < 4; ++ax1_2) {
        for (int local_id = 0; local_id < 8; ++local_id) {
(&(((int*)buf_shmem)[((((((int)threadIdx.y) * 4096) + (((int)threadIdx.z) * 1024)) + (ax1_2 * 256)) + 5120)]))[((((((local_id % 4) / 2) * 8) + (threadIdx.x / 4)) * 16) + ((((local_id / 4) * 8) + ((threadIdx.x % 4) * 2)) + (local_id % 2)))] = C_reindex_shared_warp[((ax0 * 32) + (ax1_2 * 8)) + local_id];
}
;
      }
      __syncthreads();
      #pragma unroll
      for (int ax0_ax1_ax2_ax3_ax4_fused_0 = 0; ax0_ax1_ax2_ax3_ax4_fused_0 < 8; ++ax0_ax1_ax2_ax3_ax4_fused_0) {
        *(int4*)(C + ((((((((((((int)blockIdx.y) * 1048576) + (((int)threadIdx.y) * 524288)) + (ax0 * 262144)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 & 1) * 131072)) + ((((int)threadIdx.x) >> 2) * 16384)) + (((int)blockIdx.z) * 1024)) + (((int)blockIdx.x) * 128)) + (((int)threadIdx.z) * 64)) + ((ax0_ax1_ax2_ax3_ax4_fused_0 >> 1) * 16)) + ((((int)threadIdx.x) & 3) * 4))) = *(int4*)(((int*)buf_shmem) + (((((((int)threadIdx.y) * 4096) + (((int)threadIdx.z) * 1024)) + (ax0_ax1_ax2_ax3_ax4_fused_0 * 128)) + (((int)threadIdx.x) * 4)) + 5120));
      }
    }
  }
}


